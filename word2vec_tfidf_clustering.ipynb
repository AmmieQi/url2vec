{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from url_sequences.sequence_manager import *\n",
    "from url_sequences.sequence_plotter import *\n",
    "from url_sequences.sequence_handler import *\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "import plotly.plotly as py\n",
    "from plotly.graph_objs import *\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading no-costraint & list-costraint Dataset\n",
    "- **content_nc_map** -> dict {code: content_string}\n",
    "- **url_nc_map** -> dict {code: longurl}\n",
    "- **pages_content_nc** -> list [content strings]\n",
    "- **codes_nc** -> list [codes]\n",
    "- **longurls_nc** -> list [longurls]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"No-Costraint\"\"\"\n",
    "nocostraint_path   = os.getcwd() + \"/dataset/new/cs.illinois.eduNoConstraint.words1000.depth.10/\"\n",
    "vertex_nc_path     = nocostraint_path + \"vertex.txt\"\n",
    "map_nc_path        = nocostraint_path + \"urlsMap.txt\"\n",
    "\n",
    "# code -> content_string - dict \n",
    "content_nc_map = get_content_map(vertex_nc_path)\n",
    "# code -> longurl - dict \n",
    "url_nc_map = get_urlmap(map_nc_path)\n",
    "# document no-costraint list\n",
    "pages_content_nc = [content_nc_map[key] for key in content_nc_map]\n",
    "# codes no-costraint list\n",
    "codes_nc = [key for key in content_nc_map]\n",
    "# longurls no-costraint list\n",
    "longurls_nc = [url_nc_map[key] for key in content_nc_map]\n",
    "\n",
    "\n",
    "\"\"\"List-Costraint\"\"\"\n",
    "listcostraint_path = os.getcwd() + \"/dataset/new/cs.illinois.edu.ListConstraint.words1000.depth10/\"\n",
    "vertex_lc_path     = listcostraint_path + \"vertex.txt\"\n",
    "map_lc_path        = listcostraint_path + \"urlsMap.txt\"\n",
    "\n",
    "# code -> content_string - dict \n",
    "content_lc_map = get_content_map(vertex_lc_path)\n",
    "# code -> longurl - dict \n",
    "url_lc_map = get_urlmap(map_lc_path)\n",
    "# document list-costraint list\n",
    "pages_content_lc = [content_lc_map[key] for key in content_lc_map]\n",
    "# codes list-costraint list\n",
    "codes_lc = [key for key in content_lc_map]\n",
    "# longurls list-costraint list\n",
    "longurls_lc = [url_lc_map[key] for key in content_lc_map]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF Matrix\n",
    "Term frequency - inverse document frequency (tf-idf) vectorizer parameters.\n",
    "To get a Tf-idf matrix, first count word occurrences by document. This is transformed into a document-term matrix (dtm). This is also just called a term frequency matrix.\n",
    "\n",
    "![Alt text](http://www.codeproject.com/KB/WPF/NNMFSearchResultClusterin/table.jpg \"DTM\") \n",
    "\n",
    "Then apply the term frequency-inverse document frequency weighting: words that occur frequently within a document but not frequently within the corpus receive a higher weighting as these words are assumed to contain more meaning in relation to the document.\n",
    "\n",
    "Parameters defined below:\n",
    "\n",
    "**max_df**: this is the maximum frequency within the documents a given feature can have to be used in the tfi-idf matrix. If the term is in greater than 80% of the documents it probably cares little meanining (in the context of film synopses)\n",
    "\n",
    "**min_idf**: this could be an integer (e.g. 5) and the term would have to be in at least 5 of the documents to be considered. Here I pass 0.2; the term must be in at least 20% of the document. I found that if I allowed a lower min_df I ended up basing clustering on names--for example \"Michael\" or \"Tom\" are names found in several of the movies and the synopses use these names frequently, but the names carry no real meaning.\n",
    "\n",
    "**ngram_range**: this just means I'll look at unigrams, bigrams and trigrams. See n-grams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TFIDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_df = 0.8,\n",
    "    max_features = 200000,\n",
    "    min_df = 0.1,\n",
    "    stop_words = 'english',\n",
    "    use_idf = True,\n",
    "    tokenizer = tokenize_and_stem,\n",
    "    ngram_range = (1,3)\n",
    ")\n",
    "\n",
    "\"\"\"TFIDF matrix No-Costraint\"\"\"\n",
    "tfidf_matrix_nc = tfidf_vectorizer.fit_transform(pages_content_nc)\n",
    "\n",
    "\"\"\"TFIDF matrix List-Costraint\"\"\"\n",
    "tfidf_matrix_lc = tfidf_vectorizer.fit_transform(pages_content_lc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity\n",
    "dist is defined as 1 - the cosine similarity of each document. Cosine similarity is measured against the tf-idf matrix and can be used to generate a measure of similarity between each document and the other documents in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"No-Costraint\"\"\"\n",
    "dist_nc = 1 - cosine_similarity(tfidf_matrix_nc)\n",
    "\n",
    "\"\"\"List-Costraint\"\"\"\n",
    "dist_lc = 1 - cosine_similarity(tfidf_matrix_lc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec\n",
    "Applying word2vec (skip-gram with negative sampling) to sequences generated by crawling the web site. Vectors generated for each web-page are stored in **w2v_vecs_nc**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"No-Costraint\"\"\"\n",
    "sequences_nc = nocostraint_path + \"sequenceIDs.txt\"\n",
    "\n",
    "# because of generator\n",
    "vocab_sequences_nc = get_seq(sequences_nc, 1)\n",
    "train_sequences_nc = get_seq(sequences_nc, 1)\n",
    "\n",
    "\"\"\"List-Costraint\"\"\"\n",
    "sequences_lc = listcostraint_path + \"sequenceIDs.txt\"\n",
    "\n",
    "# because of generator\n",
    "vocab_sequences_lc = get_seq(sequences_lc, 1)\n",
    "train_sequences_lc = get_seq(sequences_lc, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"No-Costraint\"\"\"\n",
    "w2v_model_nc = Word2Vec(min_count=1, negative=5, size=48)\n",
    "w2v_model_nc.build_vocab(vocab_sequences_nc)\n",
    "w2v_model_nc.train(train_sequences_nc)\n",
    "\n",
    "w2v_vecs_nc = np.array([w2v_model_nc[key] for key in content_nc_map])\n",
    "\n",
    "\"\"\"List-Costraint\"\"\"\n",
    "w2v_model_lc = Word2Vec(min_count=1, negative=5, size=48)\n",
    "w2v_model_lc.build_vocab(vocab_sequences_lc)\n",
    "w2v_model_lc.train(train_sequences_lc)\n",
    "\n",
    "w2v_vecs_lc = np.array([w2v_model_lc[key] for key in content_lc_map])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Semantic Analysis (LSA)\n",
    "Dimensionality reduction using **truncated SVD** (aka LSA).\n",
    "This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). \n",
    "\n",
    "It is very similar to PCA, but operates on sample vectors directly, instead of on a covariance matrix.\n",
    "\n",
    "In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers. In that context, it is known as latent semantic analysis (LSA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=50, algorithm=\"arpack\", random_state=1)\n",
    "\n",
    "\"\"\"No-Costraint\"\"\"\n",
    "pages_tfidf_vecs_nc = svd.fit_transform(tfidf_matrix_nc)\n",
    "\n",
    "\"\"\"List-Costraint\"\"\"\n",
    "pages_tfidf_vecs_lc = svd.fit_transform(tfidf_matrix_lc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Vectors from word2vec and TFIDF\n",
    "Appending TFIDF vector at the end of the relative word2vec vector for each page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"No-Costraint\"\"\"\n",
    "combined_vecs_nc = [np.append(w2v_vecs_nc[i], pages_tfidf_vecs_nc[i]) for i in range(len(pages_tfidf_vecs_nc))]\n",
    "\n",
    "\"\"\"List-Costraint\"\"\"\n",
    "combined_vecs_lc = [np.append(w2v_vecs_lc[i], pages_tfidf_vecs_lc[i]) for i in range(len(pages_tfidf_vecs_lc))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-SNE\n",
    "Reducing vectors dimensionality to 2 for plot purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, random_state=1)\n",
    "\n",
    "\"\"\"No-Costraint\"\"\"\n",
    "twodim_nc = tsne.fit_transform(combined_vecs_nc)\n",
    "\n",
    "\"\"\"List-Costraint\"\"\"\n",
    "twodim_lc = tsne.fit_transform(combined_vecs_lc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=15)\n",
    "\n",
    "\"\"\"No-Costraint\"\"\"\n",
    "kmeans.fit(tfidf_matrix_nc)\n",
    "kmeans_clusters_nc = kmeans.labels_\n",
    "kmeans_colors_nc = [get_color(i) for i in kmeans_clusters_nc]\n",
    "\n",
    "\"\"\"List-Costraint\"\"\"\n",
    "kmeans.fit(tfidf_matrix_lc)\n",
    "kmeans_clusters_lc = kmeans.labels_\n",
    "kmeans_colors_lc = [get_color(i) for i in kmeans_clusters_lc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means No-costraint plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\"seamless=\"seamless\" src=\"https://plot.ly/~chrispolo/34.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans_data_nc = scatter_plot(twodim_nc, word_labels=longurls_nc, colors=kmeans_colors_nc)\n",
    "py.iplot(kmeans_data_nc, filename=\"K-Means TFIDF-W2V Clustering - No-Costraint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means List-costraint plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\"seamless=\"seamless\" src=\"https://plot.ly/~chrispolo/36.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans_data_lc = scatter_plot(twodim_lc, word_labels=longurls_lc, colors=kmeans_colors_lc)\n",
    "py.iplot(kmeans_data_lc, filename=\"K-Means TFIDF-W2V Clustering - List-Costraint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDBSCAN Clsustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clusters found with HDBSCAN in No-costraint Dataset: 9\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, -1] \n",
      "\n",
      "Clusters found with HDBSCAN in List-costraint Dataset: 9\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, -1]\n"
     ]
    }
   ],
   "source": [
    "hdbscan = HDBSCAN(min_cluster_size=7)\n",
    "\n",
    "\"\"\"No-Costraint\"\"\"\n",
    "hdbscan_labels_nc = hdbscan.fit_predict(tfidf_matrix_nc)\n",
    "hdbscan_colors_nc = [get_color(n_clust) for n_clust in hdbscan_labels_nc]\n",
    "\n",
    "print \"Clusters found with HDBSCAN in No-costraint Dataset:\", len(set(hdbscan_labels_nc))\n",
    "print [label for label in set(hdbscan_labels_nc)], \"\\n\"\n",
    "\n",
    "\"\"\"List-Costraint\"\"\"\n",
    "hdbscan_labels_lc = hdbscan.fit_predict(tfidf_matrix_lc)\n",
    "hdbscan_colors_lc = [get_color(n_clust) for n_clust in hdbscan_labels_lc]\n",
    "\n",
    "print \"Clusters found with HDBSCAN in List-costraint Dataset:\", len(set(hdbscan_labels_nc))\n",
    "print [label for label in set(hdbscan_labels_nc)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDBSCAN No-costraint plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\"seamless=\"seamless\" src=\"https://plot.ly/~chrispolo/38.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdbscan_data_nc = scatter_plot(twodim_nc, word_labels=longurls_nc, colors=hdbscan_colors_nc)\n",
    "py.iplot(hdbscan_data_nc, filename=\"HDBSCAN TFIDF-W2V Clustering - No-Costraint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDBSCAN List-costraint plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\"seamless=\"seamless\" src=\"https://plot.ly/~chrispolo/40.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdbscan_data_lc = scatter_plot(twodim_lc, word_labels=longurls_lc, colors=hdbscan_colors_lc)\n",
    "py.iplot(hdbscan_data_lc, filename=\"HDBSCAN TFIDF-W2V Clustering - List-Costraint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "X = np.random.rand(100,100)\n",
    "t = TSNE(n_components=20)\n",
    "t.fit(X)\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
