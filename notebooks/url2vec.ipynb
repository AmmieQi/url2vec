{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# url2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "#from sklearn import metrics\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "#from gensim.models import Word2Vec\n",
    "#from url2vec.util.metrics import *\n",
    "from url2vec.util.plotter import *\n",
    "from url2vec.model.urlembed import *\n",
    "from url2vec.util.seqmanager import *\n",
    "\n",
    "#from sklearn.decomposition import TruncatedSVD\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nocostraint_path = os.getcwd() + \"/../dataset/cs.illinois.edu_NoConstraint.words1000.depth10/\"\n",
    "\n",
    "vertex_path      = nocostraint_path + \"vertex.txt\"\n",
    "codecontent_map  = get_content_map(vertex_path)\n",
    "\n",
    "map_path         = nocostraint_path + \"urlsMap.txt\"\n",
    "codeurl_map      = get_urlmap(map_path)\n",
    "\n",
    "sequences_path   = nocostraint_path + \"sequenceIDs.txt\"\n",
    "sequences        = get_sequences(sequences_path)\n",
    "\n",
    "\n",
    "gt = GroundTruth()\n",
    "ground_truth = [int(gt.get_groundtruth(codeurl_map[code])) for code in codeurl_map]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Url2Vec(codeurl_map)\n",
    "\n",
    "# print [codeurl_map[i] for i in codeurl_map]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seq = [\n",
    "    [\"https://cs.illinois.edu/prospective-students/undergraduates/undergraduate-scholarships-and-awards/state-farm-computer-scienc/\",\n",
    "     \"https://cs.illinois.edu/news/torrellas-co-leads-workshop-popular-parallel-programming/\", \n",
    "     \"https://cs.illinois.edu/prospective-students/undergraduates/undergraduate-scholarships-and-awards/state-farm-computer-scienc/\"\n",
    "    ],\n",
    "    [\"https://cs.illinois.edu/news/torrellas-co-leads-workshop-popular-parallel-programming/\", \n",
    "     \"https://cs.illinois.edu/news/cs-illinois-has-strong-showing-sc13/\", \n",
    "     \"https://cs.illinois.edu/news/professor-josep-torrellas-receives-darpa-perfect-award/\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "urls = [\n",
    "    \"https://cs.illinois.edu/news/professor-josep-torrellas-receives-darpa-perfect-award/\", \n",
    "    \"https://cs.illinois.edu/news/cs-illinois-has-strong-showing-sc13/\", \n",
    "    \"https://cs.illinois.edu/news/torrellas-co-leads-workshop-popular-parallel-programming/\", \n",
    "    \"https://cs.illinois.edu/prospective-students/undergraduates/undergraduate-scholarships-and-awards/state-farm-computer-scienc/\"\n",
    "]\n",
    "\n",
    "contents = [\"bene bene bene\", \"si bene bene\", \"no ciao ciao\", \"come come come ciao\"]\n",
    "\n",
    "u = Url2Vec(urls)\n",
    "u.train(sequences=seq, contents=contents, d_dim_red=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = url2vec.train(sequences=seq, contents=codecontent_map)\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(url2vec.test(ground_truth), index=set(ground_truth), columns=set(labels))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "listcostraint_path = os.getcwd() + \"/../dataset/cs.illinois.edu_NoConstraint.words1000.depth10/\"\n",
    "vertex_path_lc      = listcostraint_path + \"vertex.txt\"\n",
    "codecontent_map_lc  = get_content_map(vertex_path_lc)\n",
    "\n",
    "map_path_lc         = listcostraint_path + \"urlsMap.txt\"\n",
    "codeurl_map_lc      = get_urlmap(map_path_lc)\n",
    "\n",
    "sequences_lc        = listcostraint_path + \"sequenceIDs.txt\"\n",
    "seq_lc              = get_sequences(sequences_lc)\n",
    "\n",
    "\n",
    "gt = GroundTruth()\n",
    "ground_truth = [int(gt.get_groundtruth(codeurl_map[code])) for code in codeurl_map]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**class url2vec.model.urlembed.Url2Vec(urls)**\n",
    "\n",
    "The model has to be initialized with a dictionary that associates to every URL a unique code or a list of URLs. the former case is preferable, because for large datasets it drops down training time.\n",
    "\n",
    "\n",
    "---\n",
    "    train(self, algorithm=HDBSCAN(min_cluster_size=7), use_w2v=True, use_tfidf=True, \n",
    "          e_sg=0, e_min_count=1, e_window=10, e_negative=5, e_size=48, sequences=None, \n",
    "          d_max_df=0.9, d_max_features=200000, d_min_df=0.05, d_dim_red=50, d_tfidf=True, \n",
    "          contents=None)\n",
    "        \n",
    "        algorithm: \n",
    "            - default = HDBSCAN(min_cluster_size=7)\n",
    "            - the chosen clustering algorithm. Is invoked the fit_predict of the passed object(here HDBSCAN) to\n",
    "              return the predicted labels.\n",
    "\n",
    "        use_w2v: \n",
    "            - default = True\n",
    "            - with True word2vec is used to cluster the URLs according to their position \n",
    "              in the sequences passed (sequences argument)\n",
    "            \n",
    "        use_tfidf :\n",
    "            - default = True\n",
    "            - with True documents (contents of the URL) are tranformed into \n",
    "              document-term matrix\n",
    "\n",
    "        e_sg:\n",
    "            - default = 0\n",
    "            - word2vec argument, defines the training algorithm for word2vec,\n",
    "              if 0 CBOW is used. With sg=1 skipgram is used\n",
    "\n",
    "        e_min_count:\n",
    "            - default = 1\n",
    "            - word2vec argument, ignore all words with total frequency lower than this\n",
    "\n",
    "        e_window: \n",
    "            - default = 10\n",
    "            - word2vec argument, is the maximum distance between the current and \n",
    "              predicted word within a sentence.\n",
    "\n",
    "        e_negative: \n",
    "            - default = 5\n",
    "            - word2vec argument, if > 0, negative sampling will be used, the int for \n",
    "              negative specifies how many “noise words” should be drawn. If set to 0,\n",
    "              no negative samping is used\n",
    "\n",
    "        e_size: \n",
    "            - default = 48\n",
    "            - word2vec argument, is the dimensionality of the feature vectors\n",
    "\n",
    "        sequences: \n",
    "            - default = None\n",
    "            - word2vec argument, iterable of sentences. Each sentence is a list of words\n",
    "              that will be used for training.\n",
    "\n",
    "        d_max_d: \n",
    "            - default = 0.9\n",
    "            - document argument, this is the maximum frequency within the documents a given\n",
    "              feature can have to be used in the tfi-idf matrix.\n",
    "\n",
    "        d_max_features: \n",
    "            - default = 200000\n",
    "\n",
    "        d_min_df: \n",
    "            - default = 0.05\n",
    "            - document argument, term would have to be in at least 5 of the documents \n",
    "              to be considered.\n",
    "\n",
    "        d_dim_red: \n",
    "            - default = 50\n",
    "            - document argument, is the dimensionality of the feature vectors\n",
    "\n",
    "        d_tfidf: \n",
    "            - default = True\n",
    "            - docuement argument, with True term frequency-inverse document frequency is applied to the \n",
    "              document-term matrix obtained (from the contents argument)\n",
    "\n",
    "        contents: \n",
    "            - default = None\n",
    "            - docuement argument. Must be a dict with the URLs as keys and relevant\n",
    "              page content as values\n",
    "\n",
    "---\n",
    "    test(ground_truth, pred_membership=None)\n",
    "        ground_truth:\n",
    "        \n",
    "        pred_membership:\n",
    "            - default = None\n",
    "\n",
    "---\n",
    "    homogeneity_score(ground_truth=None, pred_membership=None)\n",
    "        ground_truth:\n",
    "            - default = None\n",
    "        \n",
    "        pred_membership:\n",
    "            - default = None\n",
    "\n",
    "---\n",
    "    completeness_score(ground_truth=None, pred_membership=None)\n",
    "        ground_truth:\n",
    "            - default = None\n",
    "        \n",
    "        pred_membership:\n",
    "            - default = None\n",
    "---\n",
    "    v_measure_score(ground_truth=None, pred_membership=None)\n",
    "        ground_truth:\n",
    "            - default = None\n",
    "        \n",
    "        pred_membership:\n",
    "            - default = None\n",
    "---\n",
    "    adjusted_rand_score(ground_truth=None, pred_membership=None)\n",
    "        ground_truth:\n",
    "            - default = None\n",
    "        \n",
    "        pred_membership:\n",
    "            - default = None\n",
    "---\n",
    "    adjusted_mutual_info_score(ground_truth=None, pred_membership=None)\n",
    "        ground_truth:\n",
    "            - default = None\n",
    "        \n",
    "        pred_membership:\n",
    "            - default = None\n",
    "---\n",
    "    silhouette_score(pred_membership=None)\n",
    "        pred_membership:\n",
    "            - default = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seq = get_sequences(sequences)\n",
    "codecontent_map = get_content_map(vertex_path)\n",
    "\n",
    "k_memb = u2v.train(algorithm=KMeans(n_clusters=15), sequences_list=seq, codecontent_map=codecontent_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seq2 = get_sequences(sequences)\n",
    "hd_memb = u2v2.train(sequences_list=seq2, codecontent_map=codecontent_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gt = GroundTruth()\n",
    "ground_truth = [int(gt.get_groundtruth(codeurl_map[code])) for code in codeurl_map]\n",
    "\n",
    "confusion_table = u2v.test(ground_truth)\n",
    "pd.DataFrame(confusion_table, index=set(ground_truth), columns=set(u2v.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "confusion_table_hdbscan = u2v2.test(ground_truth)\n",
    "pd.DataFrame(confusion_table_hdbscan, index=set(ground_truth), columns=set(u2v2.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# k_memb  \n",
    "labels_pred_k = map(int, k_memb)\n",
    "labels_pred_h = hd_memb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Homogeneity:\\t\\t\", metrics.homogeneity_score(ground_truth, labels_pred_k)\n",
    "print \"Completeness:\\t\\t\", metrics.completeness_score(ground_truth, labels_pred_k)\n",
    "print \"V Measure Score:\\t\", metrics.v_measure_score(ground_truth, labels_pred_k)\n",
    "print \"adjusted rand score:\\t\", metrics.adjusted_rand_score(ground_truth, labels_pred_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Homogeneity:\\t\\t\", metrics.homogeneity_score(ground_truth, labels_pred_h) # best 1\n",
    "print \"Completeness:\\t\\t\", metrics.completeness_score(ground_truth, labels_pred_h)  # best 1\n",
    "print \"V Measure Score:\\t\", metrics.v_measure_score(ground_truth, labels_pred_h) # best 1\n",
    "print \"adjusted rand score:\\t\", metrics.adjusted_rand_score(ground_truth, labels_pred_h) # best 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
