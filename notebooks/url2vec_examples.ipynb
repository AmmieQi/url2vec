{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "from util.sequence_manager import *\n",
    "from util.sequence_plotter import *\n",
    "from util.sequence_handler import *\n",
    "from util.clustering_metrics import *\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from itertools import tee\n",
    "import oset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rm = RealMembership()\n",
    "rm.get_membership(\"https://cs.illinois.edu/news\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Url2Vec:\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__ (self, codeurl_map):\n",
    "        # assert (type(codeurl_map) is dict), \"url2vec needs a map that associates a code(e.g. a number) for each URL\"\n",
    "        self.labels_ = None\n",
    "        # assume is list-like or fail gracefully\n",
    "        self.codeurl_map = codeurl_map if type(codeurl_map) is dict else {str(x): codeurl_map[x] for x in range(len(codeurl_map))}\n",
    "    \n",
    "    \n",
    "    # matching matrix \n",
    "    def __get_confusion_table(self, ground_truth, clusters_found_labels):\n",
    "        assert isinstance(ground_truth[0], int), \"Type is not int\"\n",
    "        assert isinstance(clusters_found_labels[0], int), \"Type is not int\"\n",
    "    \n",
    "       # matrix(num_of real_clusters x clusters_found)\n",
    "        conf_table = np.zeros((len(set(ground_truth)), len(set(clusters_found_labels))))\n",
    "        real_clusters_set = set(ground_truth)\n",
    "\n",
    "        realLabel_index_map = {}\n",
    "        index = 0\n",
    "        for c in real_clusters_set:\n",
    "            if not c in realLabel_index_map:\n",
    "                realLabel_index_map[c] = index\n",
    "                index += 1\n",
    "        print realLabel_index_map\n",
    "\n",
    "        for current_clust in realLabel_index_map.values():\n",
    "            for i in range(len(clusters_found_labels)):\n",
    "                if realLabel_index_map[ground_truth[i]] == current_clust:\n",
    "                    cluster_found = clusters_found_labels[i]\n",
    "                    conf_table[current_clust, cluster_found] = conf_table[current_clust, cluster_found] + 1\n",
    "        return conf_table\n",
    "    \n",
    "    \n",
    "    # trains word2vec with the given parameters and returns vectors for each page\n",
    "    def __word_embedding(self, sequences_list, vecs_length=48):\n",
    "        assert hasattr(sequences_list, '__iter__'), \"Bad sequences argument\"\n",
    "        \n",
    "        w2v_model = Word2Vec(min_count=1, negative=5, size=vecs_length)\n",
    "        build_seq, train_seq = itertools.tee(sequences_list)\n",
    "        w2v_model.build_vocab(build_seq)\n",
    "        w2v_model.train(train_seq)\n",
    "        return np.array([w2v_model[code] for code in self.codeurl_map])\n",
    "        \n",
    "    \n",
    "    # returns tfidf vector for each page\n",
    "    def __tfidf(self, codecontent_map, vecs_length=50, tfidf=True):\n",
    "        assert set(codecontent_map.keys()) == set(self.codeurl_map.keys()), \"NEIN\"\n",
    "        \n",
    "        self.codecontent_map = codecontent_map\n",
    "        self.pages_content = [self.codecontent_map[code] for code in self.codeurl_map]\n",
    "        self.codes = [code for code in self.codeurl_map]\n",
    "        self.longurls = [self.codeurl_map[code] for code in self.codeurl_map]\n",
    "        \n",
    "        tfidf_vectorizer = TfidfVectorizer(\n",
    "            max_df = 0.9,\n",
    "            max_features = 200000,\n",
    "            min_df = 0.05,\n",
    "            stop_words = 'english',\n",
    "            use_idf = tfidf,\n",
    "            tokenizer = tokenize_and_stem,\n",
    "            ngram_range = (1,3)\n",
    "        )\n",
    "        tfidf_matrix = tfidf_vectorizer.fit_transform(self.pages_content)\n",
    "        svd = TruncatedSVD(n_components=vecs_length, algorithm=\"arpack\", random_state=1)\n",
    "        return svd.fit_transform(tfidf_matrix)\n",
    "    \n",
    "    \n",
    "    # calls the chosen algorithm with the data builded from the input arguments\n",
    "    def train(self, algorithm=HDBSCAN(min_cluster_size=7), use_w2v=True, use_tfidf=True,\n",
    "              w2v_size=48, tfidf_size=50, sequences_list=None, codecontent_map=None):\n",
    "        \n",
    "        assert (use_w2v and sequences_list is not None or use_tfidf and codecontent_map is not None), \"Bad arguments!\"\n",
    "        \n",
    "        empty_array = np.array([ [] for i in range(len(self.codeurl_map)) ])\n",
    "        w2v_vecs    = self.__word_embedding(sequences_list, vecs_length=w2v_size) if use_w2v else empty_array\n",
    "        tfidf_vecs  = self.__tfidf(codecontent_map, vecs_length=tfidf_size) if use_tfidf else empty_array\n",
    "        \n",
    "        data = [ np.append(w2v_vecs[i], tfidf_vecs[i]) for i in range(len(self.codeurl_map)) ]\n",
    "        self.labels_ = algorithm.fit_predict(data)\n",
    "        self.labels_ = [int(x) for x in self.labels_] # map(int, self.labels_)\n",
    "        return self.labels_\n",
    "            \n",
    "    \n",
    "    # needs the real membership (ground truth) and the membership returned by the algorithm (pred_membership)\n",
    "    # ...(already given if train was successful)\n",
    "    # returns the confusion matrix\n",
    "    def test(self, ground_truth, pred_membership=None):\n",
    "        assert (pred_membership is not None or self.labels_ is not None), \"No train, No test !\"\n",
    "        pred_membership = self.labels_ if pred_membership is None else pred_membership\n",
    "        \n",
    "        return self.__get_confusion_table(ground_truth, pred_membership)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nocostraint_path = os.getcwd() + \"/../dataset/new/cs.illinois.edu.ListConstraint.words1000.depth10/\"\n",
    "rmfile  = \"/home/chris/workspace/jupyter-notebook/url2vec/dataset/manual-membership/urlToMembership.txt\"\n",
    "\n",
    "vertex_path      = nocostraint_path + \"vertex.txt\"\n",
    "codecontent_map  = get_content_map(vertex_path)\n",
    "\n",
    "map_path         = nocostraint_path + \"urlsMap.txt\"\n",
    "codeurl_map      = get_urlmap(map_path)\n",
    "\n",
    "sequences        = nocostraint_path + \"sequenceIDs.txt\"\n",
    "seq              = get_seq(sequences)\n",
    "\n",
    "rm = RealMembership(rmfile)\n",
    "ground_truth = [int(rm.get_membership(codeurl_map[code])) for code in codeurl_map]\n",
    "\n",
    "u2vNEW = Url2Vec(codeurl_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1022\n"
     ]
    }
   ],
   "source": [
    "labels = u2vNEW.train(sequences_list=seq, codecontent_map=codecontent_map)\n",
    "print(len (labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tup = (1,2,3)\n",
    "tup[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 6: 5, 8: 6, 10: 7, 12: 8, 13: 9, 14: 10, 15: 11, -1: 12}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>16</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>128</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>27</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1   2   3   4   5   6   7   8   9    10  11   12  13  14   15\n",
       "0    0   0   0   0   0   0   0  13   0   0    0   0    0   0   0    5\n",
       "1    0   0   0   0   0   0   0   7   0   0    0   0    0   0   0    2\n",
       "2    0   0   0   0   0   0   0   0   0  49    0   0    0  19  16   85\n",
       "3    0   0   0   0  28   0   0   0   0   0    0   0    0   0   0    1\n",
       "4    0   0   0   0   0   0   9   0   0   0    0   0    0   0   0    1\n",
       "5    0   0   0   0   0   0   0   0   0   0    0   0    0   0   0    6\n",
       "6    0   0   0   0   0   0   0   0   0   0  128  36    0   0   0  127\n",
       "7    0   0   0   0   0   0   0   0  11   0    0   0  200   0   0  105\n",
       "8    0   0   0   0   0   0   0   0   0   0    0   0    0   0   0   10\n",
       "9    0   9  27  11   0   0   0   0   0   0    0   0    0   0   0   10\n",
       "10  46   0   0   0   0   0   0   0   0   0    0   0    0   0   0    0\n",
       "11   0   0   0   0   0   0   0   0   0   0    0   0    0   0   0   11\n",
       "12   0   0   0   0   0   7   0   0   0   0    1   0    0   0   0   42"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(u2vNEW.test(ground_truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://cs.illinois.edu'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listi = [\"0\", 7, \"9\", \"3\"]\n",
    "d = {str(x): listi[x] for x in range(len(listi))}\n",
    "codeurl_map[\"3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Url2Vecccz:\n",
    "    # Real Membership default file path\n",
    "    # doesn't work on ipython because of __file__\n",
    "    # __rmfile = os.path.abspath(os.path.dirname(__file__)) + \"../dataset/manual-membership/urlToMembership.txt\"\n",
    "    # __rmfile  = \"/home/chris/workspace/jupyter-notebook/url2vec/dataset/manual-membership/urlToMembership.txt\"\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__ (self, map_file):\n",
    "        self.labels_ = None\n",
    "        self.codeurl_map = get_urlmap(map_file)\n",
    "    \n",
    "    # \n",
    "    def __get_confusion_table(self, real_membership_list, clusters_found_labels):\n",
    "        assert isinstance(real_membership_list[0], int), \"Type is not int\"\n",
    "        assert isinstance(clusters_found_labels[0], int), \"Type is not int\"\n",
    "    \n",
    "       # matrix(num_of real_clusters x clusters_found)\n",
    "        conf_table = np.zeros((len(set(real_membership_list)), len(set(clusters_found_labels))))\n",
    "        real_clusters_set = set(real_membership_list)\n",
    "\n",
    "        realLabel_index_map = {}\n",
    "        index = 0\n",
    "        for c in real_clusters_set:\n",
    "            if not c in realLabel_index_map:\n",
    "                realLabel_index_map[c] = index\n",
    "                index += 1\n",
    "        print realLabel_index_map\n",
    "\n",
    "        for current_clust in realLabel_index_map.values():\n",
    "            for i in range(len(clusters_found_labels)):\n",
    "                if realLabel_index_map[real_membership_list[i]] == current_clust:\n",
    "                    cluster_found = clusters_found_labels[i]\n",
    "                    conf_table[current_clust, cluster_found] = conf_table[current_clust, cluster_found] + 1\n",
    "        return conf_table\n",
    "    \n",
    "    \n",
    "    # trains word2vec with the given parameters and returns vectors for each page\n",
    "    def __word_embedding(self, sequences_file, vecs_length=48):\n",
    "        vocab_sequences = get_seq(sequences_file, 1)\n",
    "        train_sequences = get_seq(sequences_file, 1)\n",
    "        \n",
    "        w2v_model = Word2Vec(min_count=1, negative=5, size=vecs_length)\n",
    "        w2v_model.build_vocab(vocab_sequences)\n",
    "        w2v_model.train(train_sequences)\n",
    "        return np.array([w2v_model[code] for code in self.codeurl_map])\n",
    "        \n",
    "    \n",
    "    # returns tfidf vector for each page\n",
    "    def __tfidf(self, vertex_file, vecs_length=50, tfidf=True):\n",
    "        self.codecontent_map = get_content_map(vertex_file)\n",
    "        self.pages_content = [self.codecontent_map[code] for code in self.codeurl_map]\n",
    "        self.codes = [code for code in self.codeurl_map]\n",
    "        self.longurls = [self.codeurl_map[code] for code in self.codeurl_map]\n",
    "        \n",
    "        tfidf_vectorizer = TfidfVectorizer(\n",
    "            max_df = 0.9,\n",
    "            max_features = 200000,\n",
    "            min_df = 0.05,\n",
    "            stop_words = 'english',\n",
    "            use_idf = tfidf,\n",
    "            tokenizer = tokenize_and_stem,\n",
    "            ngram_range = (1,3)\n",
    "        )\n",
    "        tfidf_matrix = tfidf_vectorizer.fit_transform(self.pages_content)\n",
    "        svd = TruncatedSVD(n_components=vecs_length, algorithm=\"arpack\", random_state=1)\n",
    "        return svd.fit_transform(tfidf_matrix)\n",
    "    \n",
    "    \n",
    "    # calls the chosen algorithm with the data builded from the input arguments\n",
    "    def train(self, algorithm=HDBSCAN(min_cluster_size=10), use_w2v=True, use_tfidf=True,\n",
    "              w2v_size=48, tfidf_size=50, sequences_file=\"\", vertex_file=\"\"):\n",
    "        \n",
    "        empty_array = np.array([ [] for i in range(len(self.codeurl_map)) ])\n",
    "        w2v_vecs    = self.__word_embedding(sequences_file, vecs_length=w2v_size) if use_w2v else empty_array\n",
    "        tfidf_vecs  = self.__tfidf(vertex_file, vecs_length=tfidf_size) if use_tfidf else empty_array\n",
    "        \n",
    "        data = [ np.append(w2v_vecs[i], tfidf_vecs[i]) for i in range(len(self.codeurl_map)) ]\n",
    "        self.labels_ = algorithm.fit_predict(data)\n",
    "        self.labels_ = [int(x) for x in self.labels_] # map(int, self.labels_)\n",
    "        return self.labels_\n",
    "            \n",
    "    \n",
    "    # needs the real membership file and the membership returned by the algorithm \n",
    "    # (already given if train was successful)\n",
    "    # returns the confusion matrix\n",
    "    def test(self, realmembership_file, alg_membership=None):\n",
    "        assert (alg_membership is not None or self.labels_ is not None), \"No train, No test !\"\n",
    "        alg_membership = self.labels_ if alg_membership is None else alg_membership\n",
    "        rm = RealMembership(fpath=realmembership_file)\n",
    "        real_membership = [int(rm.get_membership(self.codeurl_map[code])) for code in self.codeurl_map]\n",
    "        \n",
    "        return self.__get_confusion_table(real_membership, alg_membership)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nocostraint_path = os.getcwd() + \"/../dataset/new/cs.illinois.edu.ListConstraint.words1000.depth10/\"\n",
    "rmfile  = \"/home/chris/workspace/jupyter-notebook/url2vec/dataset/manual-membership/urlToMembership.txt\"\n",
    "vertex_path      = nocostraint_path + \"vertex.txt\"\n",
    "map_path         = nocostraint_path + \"urlsMap.txt\"\n",
    "sequences        = nocostraint_path + \"sequenceIDs.txt\"\n",
    "\n",
    "u2v  = Url2Vec(map_path)\n",
    "u2v2 = Url2Vec(map_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'u2v' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-032b55d07732>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# vec2 = u2v.word_embedding(sequences)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mk_memb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mu2v\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mKMeans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msequences_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msequences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertex_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvertex_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'u2v' is not defined"
     ]
    }
   ],
   "source": [
    "# vec = u2v._Url2Vec__tfidf()\n",
    "# vec2 = u2v.word_embedding(sequences)\n",
    "\n",
    "k_memb = u2v.train(algorithm=KMeans(n_clusters=15), sequences_file=sequences, vertex_file=vertex_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hd_memb = u2v2.train(sequences_file=sequences, vertex_file=vertex_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "confusion_table = u2v.test(rmfile)\n",
    "pd.DataFrame(confusion_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "confusion_table_hdbscan = u2v2.test(rmfile)\n",
    "pd.DataFrame(confusion_table_hdbscan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'k_memb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-2ac8febb1bfa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# k_memb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mlabels_pred_k\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk_memb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mlabels_pred_h\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhd_memb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mcodeurl_map\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_urlmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'k_memb' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "# k_memb  \n",
    "labels_pred_k = map(int, k_memb)\n",
    "labels_pred_h = hd_memb\n",
    "codeurl_map = get_urlmap(map_path)\n",
    "rm = RealMembership(rmfile)\n",
    "labels_true = [int(rm.get_membership(codeurl_map[code])) for code in codeurl_map]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Homogeneity:\", metrics.homogeneity_score(labels_true, labels_pred_k)\n",
    "print \"Completeness:\", metrics.completeness_score(labels_true, labels_pred_k)\n",
    "print \"V Measure Score:\", metrics.v_measure_score(labels_true, labels_pred_k)\n",
    "print \"adjusted rand score:\", metrics.adjusted_rand_score(labels_true, labels_pred_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Homogeneity:\", metrics.homogeneity_score(labels_true, labels_pred_h) # best 1\n",
    "print \"Completeness:\", metrics.completeness_score(labels_true, labels_pred_h)  # best 1\n",
    "print \"V Measure Score:\", metrics.v_measure_score(labels_true, labels_pred_h) # best 1\n",
    "print \"adjusted rand score\", metrics.adjusted_rand_score(labels_true, labels_pred_h) # best 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
