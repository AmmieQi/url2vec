{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "import plotly.plotly as py\n",
    "from plotly.graph_objs import *\n",
    "\n",
    "from urlembed.util.plotter import *\n",
    "from urlembed.util.seqmanager import *\n",
    "\n",
    "from sklearn import metrics\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from __future__ import print_function\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nocostraint_path = os.getcwd() + \"/../dataset/cs.illinois.edu_NoConstraint.words1000.depth10/\"\n",
    "vertex_nc_path   = nocostraint_path + \"vertex.txt\"\n",
    "map_nc_path      = nocostraint_path + \"urlsMap.txt\"\n",
    "\n",
    "codecontent_map_nc = get_content_map(vertex_nc_path)\n",
    "urlmap_nc = get_urlmap(map_nc_path)\n",
    "\n",
    "# document list\n",
    "documents_nc = [codecontent_map_nc[key] for key in codecontent_map_nc]\n",
    "codes_nc     = [key for key in codecontent_map_nc]\n",
    "urls_nc      = [urlmap_nc[key] for key in codecontent_map_nc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terms Documents Matrix\n",
    "\n",
    "![Alt text](http://www.codeproject.com/KB/WPF/NNMFSearchResultClusterin/table.jpg \"Very nice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "max_df: \n",
    "    this is the maximum frequency within the documents a given feature\n",
    "    can have to be used in the tfi-idf matrix.\n",
    "min_idf:\n",
    "    this could be an integer (e.g. 0.2) and the term would have to be in\n",
    "    at least 20% of the documents to be considered.\n",
    "ngram_range:\n",
    "    (e.g. 1,3) this just means I'll look at unigrams, bigrams and trigrams. \n",
    "\"\"\"\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_df = 0.8,\n",
    "    max_features = 200000,\n",
    "    min_df = 0.1,\n",
    "    stop_words = 'english',\n",
    "    use_idf = True,\n",
    "    tokenizer = tokenize_and_stem,\n",
    "    ngram_range = (1,3)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              \n",
      "documents  728\n",
      "terms      433\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfidf_matrix_nc = tfidf_vectorizer.fit_transform(documents_nc)\n",
    "# can be used to generate a measure of similarity between each document and the other documents in the corpus\n",
    "dist_nc = 1 - cosine_similarity(tfidf_matrix_nc)\n",
    "\n",
    "print(pd.DataFrame({\"documents\":tfidf_matrix_nc.shape[0], \"terms\":tfidf_matrix_nc.shape[1]}, index=[\"\"]).T)\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMEANS Clustering - No Costraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=15)\n",
    "kmeans_labels_nc = kmeans.fit_predict(tfidf_matrix_nc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>john deere scholarship in computer science dep...</td>\n",
       "      <td>344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jump trading scholars department of computer s...</td>\n",
       "      <td>345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rockwell collins scholarship department of com...</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spot trading scholarship department of compute...</td>\n",
       "      <td>347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>illinois cyber security scholars program icssp...</td>\n",
       "      <td>340</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            document code\n",
       "0  john deere scholarship in computer science dep...  344\n",
       "0  jump trading scholars department of computer s...  345\n",
       "0  rockwell collins scholarship department of com...  346\n",
       "0  spot trading scholarship department of compute...  347\n",
       "0  illinois cyber security scholars program icssp...  340"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_nc = { \n",
    "    'code': codes_nc,\n",
    "    'document': documents_nc\n",
    "}\n",
    "frame_nc = pd.DataFrame(docs_nc, index=[kmeans_labels_nc] , columns=['document', 'code'])\n",
    "\n",
    "frame_nc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 - top words: ->  undergraduate, undergraduate, scholarship, programs, students, advising,\n",
      "Cluster 1 - top words: ->  office, office, contact, contact, directory, staff,\n",
      "Cluster 2 - top words: ->  security, information, s, work, using, professor,\n",
      "Cluster 3 - top words: ->  networks, real, time, s, work, mobile,\n",
      "Cluster 4 - top words: ->  applications, graduate, programs, fellowship, ms, deadlines,\n",
      "Cluster 5 - top words: ->  awards, distinguished, chair, member, s, siebel,\n",
      "Cluster 6 - top words: ->  curriculum, research, news, news, offered, class,\n",
      "Cluster 7 - top words: ->  data, information, s, projects, using, said,\n",
      "Cluster 8 - top words: ->  s, moone, media, said, professor, work,\n",
      "Cluster 9 - top words: ->  papers, best, best, papers, work, awards,\n",
      "Cluster 10 - top words: ->  siebel, siebel, center, office, resources, history,\n",
      "Cluster 11 - top words: ->  win, awards, scholars, home, directory, directory,\n",
      "Cluster 12 - top words: ->  thesis, ph, ph, requirement, d, ms,\n",
      "Cluster 13 - top words: ->  parallel, performance, parallel, software, professor, programs,\n",
      "Cluster 14 - top words: ->  s, school, programs, said, team, work,\n"
     ]
    }
   ],
   "source": [
    "# map -> {code: token_list}\n",
    "tokens_nc_map = to_tokens_map(codecontent_map_nc)\n",
    "# map -> {code: stem_list}\n",
    "stems_nc_map = to_stems_map(codecontent_map_nc)\n",
    "\n",
    "# total vocabulary, list of tokens\n",
    "totalvocab_nc_stemmed = [stem for key in codecontent_map_nc for stem in stems_nc_map[key]]\n",
    "# total vocabulary, list of stems\n",
    "totalvocab_nc_tokenized = [stem for key in codecontent_map_nc for stem in tokens_nc_map[key]]\n",
    "\n",
    "vocab_nc_frame = pd.DataFrame({'words': totalvocab_nc_tokenized}, index = totalvocab_nc_stemmed)\n",
    "terms_nc = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# sort cluster centers by proximity to centroid\n",
    "order_centroids = kmeans.cluster_centers_.argsort()[:,::-1]\n",
    "\n",
    "for i in range(len(set(kmeans_labels_nc))):\n",
    "    print(\"Cluster %d - top words:\" % i, end=' -> ')\n",
    "    \n",
    "    for ind in order_centroids[i,: 6]: # replace 6 with n words per cluster\n",
    "        print(' %s' % vocab_nc_frame.ix[terms_nc[ind].split(' ')].values.tolist()[0][0].encode('utf-8', 'ignore'), end=',')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Plot - TSNE No Costraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\"seamless=\"seamless\" src=\"https://plot.ly/~chrispolo/70.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsne = TSNE(n_components=2, random_state=1)\n",
    "\n",
    "tfidf_matrix_dense_nc = tfidf_matrix_nc.todense()\n",
    "docs_vecs_nc = np.array([tfidf_matrix_dense_nc[i].A1 for i in range(len(tfidf_matrix_dense_nc))])\n",
    "clusters_colors_nc = [ get_color(i) for i in kmeans_labels_nc]\n",
    "\n",
    "twodim_docs_nc = tsne.fit_transform(dist_nc)\n",
    "\n",
    "k_tsne_data = scatter_plot(twodim_docs_nc, word_labels=urls_nc, colors=clusters_colors_nc)\n",
    "py.iplot(k_tsne_data, filename=\"K-Means t-SNE nocostraint - Doc Clustering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clustering - List Costraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "listcostraint_path = os.getcwd() + \"/../dataset/cs.illinois.edu_ListConstraint.words1000.depth10/\"\n",
    "vertex_lc_path     = listcostraint_path + \"vertex.txt\"\n",
    "map_lc_path        = listcostraint_path + \"urlsMap.txt\"\n",
    "\n",
    "codecontent_map_lc = get_content_map(vertex_lc_path)\n",
    "urlmap_lc = get_urlmap(map_lc_path)\n",
    "\n",
    "# document list\n",
    "documents_lc = [codecontent_map_lc[key] for key in codecontent_map_lc]\n",
    "codes_lc     = [key for key in codecontent_map_lc]\n",
    "urls_lc      = [urlmap_lc[key] for key in codecontent_map_lc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               \n",
      "documents  1022\n",
      "terms       370\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfidf_matrix_lc = tfidf_vectorizer.fit_transform(documents_lc) \n",
    "\n",
    "# can be used to generate a measure of similarity between each document and the other documents in the corpus\n",
    "dist_lc = 1 - cosine_similarity(tfidf_matrix_lc)\n",
    "\n",
    "print(pd.DataFrame({\"documents\":tfidf_matrix_lc.shape[0], \"terms\":tfidf_matrix_lc.shape[1]}, index=[\"\"]).T)\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=15)\n",
    "kmeans_labels_lc = kmeans.fit_predict(tfidf_matrix_lc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>engineering at illinois my cs illinois educomp...</td>\n",
       "      <td>344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>engineering at illinois my cs illinois educomp...</td>\n",
       "      <td>345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>engineering at illinois my cs illinois educomp...</td>\n",
       "      <td>346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>engineering at illinois my cs illinois educomp...</td>\n",
       "      <td>347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>engineering at illinois my cs illinois educomp...</td>\n",
       "      <td>340</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             document code\n",
       "3   engineering at illinois my cs illinois educomp...  344\n",
       "3   engineering at illinois my cs illinois educomp...  345\n",
       "0   engineering at illinois my cs illinois educomp...  346\n",
       "13  engineering at illinois my cs illinois educomp...  347\n",
       "7   engineering at illinois my cs illinois educomp...  340"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_lc = {\n",
    "    'code': codes_lc,\n",
    "    'document': documents_lc\n",
    "}\n",
    "\n",
    "frame_lc = pd.DataFrame(docs_lc, index = [kmeans_labels_lc] , columns = ['document', 'code'])\n",
    "frame_lc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 words: primary, primary, primary, research, professor, offices,\n",
      "Cluster 1 words: security, s, networking, information, programming, data,\n",
      "Cluster 2 words: undergraduate, undergraduate, programming, illinois, edu, students,\n",
      "Cluster 3 words: honors, publications, honors, contacts, contacts, offices,\n",
      "Cluster 4 words: description, topic, section, courses, curriculum, instructor,\n",
      "Cluster 5 words: ph, ph, required, ms, graduate, form,\n",
      "Cluster 6 words: b, c, sci, r, comp, comp,\n",
      "Cluster 7 words: edu, edu, illinois, center, comp, center,\n",
      "Cluster 8 words: said, s, data, used, working, development,\n",
      "Cluster 9 words: applications, programming, graduate, ms, fellowship, degree,\n",
      "Cluster 10 words: media, moone, s, working, awarded, networking,\n",
      "Cluster 11 words: parallel, performance, parallel, programming, modeling, high,\n",
      "Cluster 12 words: siebel, siebel, center, resources, edu, illinois,\n",
      "Cluster 13 words: store, courses, store, curriculum, cs, courses,\n",
      "Cluster 14 words: awarded, members, s, service, outstanding, siebel,\n"
     ]
    }
   ],
   "source": [
    "# map -> {code: token_list}\n",
    "tokens_lc_map = to_tokens_map(codecontent_map_lc)\n",
    "# map -> {code: stem_list}\n",
    "stems_lc_map = to_stems_map(codecontent_map_lc)\n",
    "\n",
    "# total vocabulary, list of tokens\n",
    "totalvocab_lc_stemmed = [stem for key in codecontent_map_lc for stem in stems_lc_map[key]]\n",
    "# total vocabulary, list of stems\n",
    "totalvocab_lc_tokenized = [stem for key in codecontent_map_lc for stem in tokens_lc_map[key]]\n",
    "\n",
    "vocab_lc_frame = pd.DataFrame({'words': totalvocab_lc_tokenized}, index = totalvocab_lc_stemmed)\n",
    "terms_lc = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# sort cluster centers by proximity to centroid\n",
    "order_centroids_lc = kmeans.cluster_centers_.argsort()[:, ::-1]\n",
    "\n",
    "for i in range(len(set(kmeans_labels_lc))):\n",
    "    print(\"Cluster %d words:\" % i, end='')\n",
    "    \n",
    "    for ind in order_centroids_lc[i,:6]: # replace 6 with n words per cluster\n",
    "        print(' %s' % vocab_lc_frame.ix[terms_lc[ind].split(' ')].values.tolist()[0][0].encode('utf-8', 'ignore'), end=',')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KMEANS Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\"seamless=\"seamless\" src=\"https://plot.ly/~chrispolo/74.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_matrix_lc_dense = tfidf_matrix_lc.todense()\n",
    "docs_vecs_lc = np.array([tfidf_matrix_lc_dense[i].A1 for i in range(len(tfidf_matrix_lc_dense))])\n",
    "\n",
    "clusters_colors_lc = [ get_color(i) for i in kmeans_labels_lc]\n",
    "\n",
    "twodim_docs_lc = tsne.fit_transform(dist_lc)\n",
    "\n",
    "k_tsne_data_lc = scatter_plot(twodim_docs_lc, word_labels=urls_lc, colors=clusters_colors_lc)\n",
    "py.iplot(k_tsne_data_lc, filename=\"K-Means listcostraint - Doc Clustering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GROUND TRUTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clusters found manually: 14\n",
      "[0, 1, 2, 3, 4, 6, 8, 10, 11, 12, 13, 14, 15, -1]\n",
      "\n",
      "Clusters found manually: 13\n",
      "[0, 1, 2, 3, 4, 6, 8, 10, 12, 13, 14, 15, -1]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gt = GroundTruth(os.getcwd() + \"/../dataset/ground_truth/urlToMembership.txt\")\n",
    "ground_truth_lc = [int(gt.get_groundtruth(urlmap_lc[key])) for key in codecontent_map_lc]\n",
    "\n",
    "gt = GroundTruth(os.getcwd() + \"/../dataset/ground_truth/urlToMembership.txt\")\n",
    "ground_truth_nc = [int(gt.get_groundtruth(urlmap_nc[key])) for key in codecontent_map_nc]\n",
    "\n",
    "print(\"Clusters found manually:\", len(set(ground_truth_nc)))\n",
    "print([label for label in set(ground_truth_nc)])\n",
    "print()\n",
    "print(\"Clusters found manually:\", len(set(ground_truth_lc)))\n",
    "print([label for label in set(ground_truth_lc)])\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN and HDBSCAN nocostraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clusters found with DBSCAN: 16\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, -1]\n",
      "\n",
      "\n",
      "Clusters found with HDBSCAN: 15\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, -1]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dbscan = DBSCAN(eps=0.9, min_samples=4)\n",
    "dbscan_labels_nc = dbscan.fit_predict(tfidf_matrix_nc)\n",
    "\n",
    "print(\"Clusters found with DBSCAN:\", len(set(dbscan_labels_nc)))\n",
    "print ([label for label in set(dbscan_labels_nc)])\n",
    "print(\"\\n\")\n",
    "\n",
    "hdbscan = HDBSCAN(min_cluster_size=4)\n",
    "hdbscan_labels_nc = hdbscan.fit_predict(tfidf_matrix_nc)\n",
    "\n",
    "print(\"Clusters found with HDBSCAN:\", len(set(hdbscan_labels_nc)))\n",
    "print([label for label in set(hdbscan_labels_nc)])\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN and HDBSCAN listcostraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clusters found with DBSCAN: 14\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, -1]\n",
      "\n",
      "\n",
      "Clusters found with HDBSCAN: 13\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, -1]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dbscan = DBSCAN(eps=0.7, min_samples=4)\n",
    "dbscan_labels_lc = dbscan.fit_predict(tfidf_matrix_lc)\n",
    "\n",
    "print(\"Clusters found with DBSCAN:\", len(set(dbscan_labels_lc)))\n",
    "print ([label for label in set(dbscan_labels_lc)])\n",
    "print(\"\\n\")\n",
    "\n",
    "hdbscan = HDBSCAN(min_cluster_size=7)\n",
    "hdbscan_labels_lc = hdbscan.fit_predict(tfidf_matrix_lc)\n",
    "\n",
    "print(\"Clusters found with HDBSCAN:\", len(set(hdbscan_labels_lc)))\n",
    "print([label for label in set(hdbscan_labels_lc)])\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Homogeneity</th>\n",
       "      <th>Completeness</th>\n",
       "      <th>V-Measure core</th>\n",
       "      <th>Adjusted Rand index</th>\n",
       "      <th>Mutual Information</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NoCostraint - DBSCAN</th>\n",
       "      <td>0.560068</td>\n",
       "      <td>0.596247</td>\n",
       "      <td>0.577592</td>\n",
       "      <td>0.407829</td>\n",
       "      <td>0.534608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NoCostraint - HDBSCAN</th>\n",
       "      <td>0.515236</td>\n",
       "      <td>0.602887</td>\n",
       "      <td>0.555626</td>\n",
       "      <td>0.386213</td>\n",
       "      <td>0.485780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NoCostraint - K-MEANS</th>\n",
       "      <td>0.754334</td>\n",
       "      <td>0.572492</td>\n",
       "      <td>0.650952</td>\n",
       "      <td>0.311199</td>\n",
       "      <td>0.548965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ListCostraint - DBSCAN</th>\n",
       "      <td>0.570969</td>\n",
       "      <td>0.704237</td>\n",
       "      <td>0.630639</td>\n",
       "      <td>0.519670</td>\n",
       "      <td>0.556604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ListCostraint - HDBSCAN</th>\n",
       "      <td>0.450059</td>\n",
       "      <td>0.510352</td>\n",
       "      <td>0.478313</td>\n",
       "      <td>0.193805</td>\n",
       "      <td>0.429703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ListCostraint - K-MEANS</th>\n",
       "      <td>0.844222</td>\n",
       "      <td>0.606859</td>\n",
       "      <td>0.706127</td>\n",
       "      <td>0.440011</td>\n",
       "      <td>0.593167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Homogeneity  Completeness  V-Measure core  \\\n",
       "NoCostraint - DBSCAN        0.560068      0.596247        0.577592   \n",
       "NoCostraint - HDBSCAN       0.515236      0.602887        0.555626   \n",
       "NoCostraint - K-MEANS       0.754334      0.572492        0.650952   \n",
       "ListCostraint - DBSCAN      0.570969      0.704237        0.630639   \n",
       "ListCostraint - HDBSCAN     0.450059      0.510352        0.478313   \n",
       "ListCostraint - K-MEANS     0.844222      0.606859        0.706127   \n",
       "\n",
       "                         Adjusted Rand index  Mutual Information  \n",
       "NoCostraint - DBSCAN                0.407829            0.534608  \n",
       "NoCostraint - HDBSCAN               0.386213            0.485780  \n",
       "NoCostraint - K-MEANS               0.311199            0.548965  \n",
       "ListCostraint - DBSCAN              0.519670            0.556604  \n",
       "ListCostraint - HDBSCAN             0.193805            0.429703  \n",
       "ListCostraint - K-MEANS             0.440011            0.593167  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_df = pd.DataFrame([\n",
    "        [\n",
    "            # dbscan nocostraint\n",
    "            metrics.homogeneity_score(ground_truth_nc, dbscan_labels_nc),\n",
    "            metrics.completeness_score(ground_truth_nc, dbscan_labels_nc),\n",
    "            metrics.v_measure_score(ground_truth_nc, dbscan_labels_nc),\n",
    "            metrics.adjusted_rand_score(ground_truth_nc, dbscan_labels_nc),\n",
    "            metrics.adjusted_mutual_info_score(ground_truth_nc, dbscan_labels_nc)\n",
    "        ],\n",
    "        [\n",
    "            # hdbscan nocostraint\n",
    "            metrics.homogeneity_score(ground_truth_nc, hdbscan_labels_nc),\n",
    "            metrics.completeness_score(ground_truth_nc, hdbscan_labels_nc),\n",
    "            metrics.v_measure_score(ground_truth_nc, hdbscan_labels_nc),\n",
    "            metrics.adjusted_rand_score(ground_truth_nc, hdbscan_labels_nc),\n",
    "            metrics.adjusted_mutual_info_score(ground_truth_nc, hdbscan_labels_nc)\n",
    "        ],\n",
    "        [\n",
    "            # kmeans nocostraint\n",
    "            metrics.homogeneity_score(ground_truth_nc, kmeans_labels_nc),\n",
    "            metrics.completeness_score(ground_truth_nc, kmeans_labels_nc),\n",
    "            metrics.v_measure_score(ground_truth_nc, kmeans_labels_nc),\n",
    "            metrics.adjusted_rand_score(ground_truth_nc, kmeans_labels_nc),\n",
    "            metrics.adjusted_mutual_info_score(ground_truth_nc, kmeans_labels_nc)\n",
    "        ],\n",
    "        [\n",
    "            # dbscan listcostraint\n",
    "            metrics.homogeneity_score(ground_truth_lc, dbscan_labels_lc),\n",
    "            metrics.completeness_score(ground_truth_lc, dbscan_labels_lc),\n",
    "            metrics.v_measure_score(ground_truth_lc, dbscan_labels_lc),\n",
    "            metrics.adjusted_rand_score(ground_truth_lc, dbscan_labels_lc),\n",
    "            metrics.adjusted_mutual_info_score(ground_truth_lc, dbscan_labels_lc)\n",
    "        ],\n",
    "        [\n",
    "            # hdbscan listcostraint\n",
    "            metrics.homogeneity_score(ground_truth_lc, hdbscan_labels_lc),\n",
    "            metrics.completeness_score(ground_truth_lc, hdbscan_labels_lc),\n",
    "            metrics.v_measure_score(ground_truth_lc, hdbscan_labels_lc),\n",
    "            metrics.adjusted_rand_score(ground_truth_lc, hdbscan_labels_lc),\n",
    "            metrics.adjusted_mutual_info_score(ground_truth_lc, hdbscan_labels_lc)\n",
    "        ],\n",
    "        [\n",
    "            # kmeans listcostraint\n",
    "            metrics.homogeneity_score(ground_truth_lc, kmeans_labels_lc),\n",
    "            metrics.completeness_score(ground_truth_lc, kmeans_labels_lc),\n",
    "            metrics.v_measure_score(ground_truth_lc, kmeans_labels_lc),\n",
    "            metrics.adjusted_rand_score(ground_truth_lc, kmeans_labels_lc),\n",
    "            metrics.adjusted_mutual_info_score(ground_truth_lc, kmeans_labels_lc)\n",
    "        ]],\n",
    "        index=[\"NoCostraint - DBSCAN\", \"NoCostraint - HDBSCAN\", \"NoCostraint - K-MEANS\", \"ListCostraint - DBSCAN\", \"ListCostraint - HDBSCAN\", \"ListCostraint - K-MEANS\"],\n",
    "        columns=[\"Homogeneity\", \"Completeness\", \"V-Measure core\", \"Adjusted Rand index\", \"Mutual Information\"])\n",
    "\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# w2v - tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from url_sequences.sequence_handler import *\n",
    "sequences_lc = listcostraint_path + \"sequenceIDs.txt\"\n",
    "\n",
    "# because of generator\n",
    "vocab_sequences = get_seq(sequences_lc, 1)\n",
    "train_sequences = get_seq(sequences_lc, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "w2v_model = Word2Vec(min_count=1, negative=5, size=48)\n",
    "w2v_model.build_vocab(vocab_sequences)\n",
    "w2v_model.train(train_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w2v_vecs_lc = np.array([w2v_model[key] for key in content_lc_map]) \n",
    "docs_vecs_lc = docs_vecs_lc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(docs_vecs_lc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tsne50 =  TSNE(n_components=50)\n",
    "docs_vecs_lc_reduced = tsne50.fit_transform(docs_vecs_lc)\n",
    "# vecs = [ np.concatenate((w2v_vecs_lc[i], docs_vecs_lc_reduced[i]), axis=0) for i in range(3)]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
