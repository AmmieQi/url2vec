{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Path of real membership file: ', '/home/chris/workspace/jupyter-notebook/url2vec/util/../dataset/manual-membership/urlToMembership.txt')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "from util.sequence_manager import *\n",
    "from util.sequence_plotter import *\n",
    "from util.sequence_handler import *\n",
    "from util.clustering_metrics import *\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "import plotly.plotly as py\n",
    "from plotly.graph_objs import *\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading no-costraint & list-costraint Dataset\n",
    "- **content_nc_map** -> dict {code: content_string}\n",
    "- **url_nc_map** -> dict {code: longurl}\n",
    "\n",
    "\n",
    "- **pages_content_nc** -> list [content strings]\n",
    "- **codes_nc** -> list [codes]\n",
    "- **longurls_nc** -> list [longurls]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: '/home/chris/workspace/jupyter-notebook/url2vec/../dataset/manual-membership/urlToMembership.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-5359eae7c9b4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;34m\"\"\"Real membership List\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mrm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRealMembership\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;34m\"\"\"No-Costraint\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/chris/workspace/jupyter-notebook/url2vec/url_sequences/sequence_manager.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, fpath, sep)\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[0mutm_map\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m             \u001b[0mkv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[0mutm_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: [Errno 2] No such file or directory: '/home/chris/workspace/jupyter-notebook/url2vec/../dataset/manual-membership/urlToMembership.txt'"
     ]
    }
   ],
   "source": [
    "\"\"\"Real membership List\"\"\"\n",
    "rm = RealMembership() \n",
    "\n",
    "\n",
    "\"\"\"No-Costraint\"\"\"\n",
    "nocostraint_path   = os.getcwd() + \"/dataset/new/cs.illinois.eduNoConstraint.words1000.depth.10/\"\n",
    "vertex_nc_path     = nocostraint_path + \"vertex.txt\"\n",
    "map_nc_path        = nocostraint_path + \"urlsMap.txt\"\n",
    "\n",
    "# code -> content_string - dict \n",
    "content_nc_map = get_content_map(vertex_nc_path)\n",
    "# code -> longurl - dict \n",
    "url_nc_map = get_urlmap(map_nc_path)\n",
    "# document no-costraint list\n",
    "pages_content_nc = [content_nc_map[key] for key in content_nc_map]\n",
    "# codes no-costraint list\n",
    "codes_nc = [key for key in content_nc_map]\n",
    "# longurls no-costraint list\n",
    "longurls_nc = [url_nc_map[key] for key in content_nc_map]\n",
    "# real membership list-costraint\n",
    "real_membership_nc = [int(rm.get_membership(url_nc_map[key])) for key in content_nc_map]\n",
    "\n",
    "\"\"\"List-Costraint\"\"\"\n",
    "listcostraint_path = os.getcwd() + \"/dataset/new/cs.illinois.edu.ListConstraint.words1000.depth10/\"\n",
    "vertex_lc_path     = listcostraint_path + \"vertex.txt\"\n",
    "map_lc_path        = listcostraint_path + \"urlsMap.txt\"\n",
    "\n",
    "# code -> content_string - dict \n",
    "content_lc_map = get_content_map(vertex_lc_path)\n",
    "# code -> longurl - dict \n",
    "url_lc_map = get_urlmap(map_lc_path)\n",
    "# document list-costraint list\n",
    "pages_content_lc = [content_lc_map[key] for key in content_lc_map]\n",
    "# codes list-costraint list\n",
    "codes_lc = [key for key in content_lc_map]\n",
    "# longurls list-costraint list\n",
    "longurls_lc = [url_lc_map[key] for key in content_lc_map]\n",
    "# real membership list-costraint\n",
    "real_membership_lc = [int(rm.get_membership(url_lc_map[key])) for key in content_lc_map]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF Matrix\n",
    "Term frequency - inverse document frequency (tf-idf) vectorizer parameters.\n",
    "To get a Tf-idf matrix, first count word occurrences by document. This is transformed into a document-term matrix (dtm). This is also just called a term frequency matrix.\n",
    "\n",
    "![Alt text](http://www.codeproject.com/KB/WPF/NNMFSearchResultClusterin/table.jpg \"DTM\") \n",
    "\n",
    "Then apply the term frequency-inverse document frequency weighting: words that occur frequently within a document but not frequently within the corpus receive a higher weighting as these words are assumed to contain more meaning in relation to the document.\n",
    "\n",
    "Parameters defined below:\n",
    "\n",
    "**max_df**: this is the maximum frequency within the documents a given feature can have to be used in the tfi-idf matrix. If the term is in greater than 80% of the documents it probably cares little meanining (in the context of film synopses)\n",
    "\n",
    "**min_idf**: this could be an integer (e.g. 5) and the term would have to be in at least 5 of the documents to be considered. Here I pass 0.2; the term must be in at least 20% of the document. I found that if I allowed a lower min_df I ended up basing clustering on names--for example \"Michael\" or \"Tom\" are names found in several of the movies and the synopses use these names frequently, but the names carry no real meaning.\n",
    "\n",
    "**ngram_range**: this just means I'll look at unigrams, bigrams and trigrams. See n-grams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TFIDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_df = 0.8,\n",
    "    max_features = 200000,\n",
    "    min_df = 0.1,\n",
    "    stop_words = 'english',\n",
    "    use_idf = True,\n",
    "    tokenizer = tokenize_and_stem,\n",
    "    ngram_range = (1,3)\n",
    ")\n",
    "\n",
    "\"\"\"TFIDF matrix No-Costraint\"\"\"\n",
    "tfidf_matrix_nc = tfidf_vectorizer.fit_transform(pages_content_nc)\n",
    "\n",
    "\"\"\"TFIDF matrix List-Costraint\"\"\"\n",
    "tfidf_matrix_lc = tfidf_vectorizer.fit_transform(pages_content_lc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity\n",
    "dist is defined as 1 - the cosine similarity of each document. Cosine similarity is measured against the tf-idf matrix and can be used to generate a measure of similarity between each document and the other documents in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"No-Costraint\"\"\"\n",
    "dist_nc = 1 - cosine_similarity(tfidf_matrix_nc)\n",
    "\n",
    "\"\"\"List-Costraint\"\"\"\n",
    "dist_lc = 1 - cosine_similarity(tfidf_matrix_lc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec\n",
    "Applying word2vec (skip-gram with negative sampling) to sequences generated by crawling the web site. Vectors generated for each web-page are stored in **w2v_vecs_nc** and **w2v_vecs_lc** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"No-Costraint\"\"\"\n",
    "sequences_nc = nocostraint_path + \"sequenceIDs.txt\"\n",
    "\n",
    "# because of generator\n",
    "vocab_sequences_nc = get_seq(sequences_nc, 1)\n",
    "train_sequences_nc = get_seq(sequences_nc, 1)\n",
    "\n",
    "\"\"\"List-Costraint\"\"\"\n",
    "sequences_lc = listcostraint_path + \"sequenceIDs.txt\"\n",
    "\n",
    "# because of generator\n",
    "vocab_sequences_lc = get_seq(sequences_lc, 1)\n",
    "train_sequences_lc = get_seq(sequences_lc, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"No-Costraint\"\"\"\n",
    "w2v_model_nc = Word2Vec(min_count=1, negative=5, size=48)\n",
    "w2v_model_nc.build_vocab(vocab_sequences_nc)\n",
    "w2v_model_nc.train(train_sequences_nc)\n",
    "\n",
    "w2v_vecs_nc = np.array([w2v_model_nc[key] for key in content_nc_map])\n",
    "\n",
    "\"\"\"List-Costraint\"\"\"\n",
    "w2v_model_lc = Word2Vec(min_count=1, negative=5, size=100)\n",
    "w2v_model_lc.build_vocab(vocab_sequences_lc)\n",
    "w2v_model_lc.train(train_sequences_lc)\n",
    "\n",
    "w2v_vecs_lc = np.array([w2v_model_lc[key] for key in content_lc_map])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Semantic Analysis (LSA)\n",
    "Dimensionality reduction using **truncated SVD** (aka LSA).\n",
    "This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). \n",
    "\n",
    "It is very similar to PCA, but operates on sample vectors directly, instead of on a covariance matrix.\n",
    "\n",
    "In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers. In that context, it is known as latent semantic analysis (LSA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=50, algorithm=\"arpack\", random_state=1)\n",
    "\n",
    "\"\"\"No-Costraint\"\"\"\n",
    "pages_tfidf_vecs_nc = svd.fit_transform(tfidf_matrix_nc)\n",
    "\n",
    "\"\"\"List-Costraint\"\"\"\n",
    "pages_tfidf_vecs_lc = svd.fit_transform(tfidf_matrix_lc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Vectors from word2vec and TFIDF\n",
    "Appending TFIDF vector at the end of the relative word2vec vector for each page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"No-Costraint\"\"\"\n",
    "combined_vecs_nc = [np.append(w2v_vecs_nc[i], pages_tfidf_vecs_nc[i]) for i in range(len(pages_tfidf_vecs_nc))]\n",
    "\n",
    "\"\"\"List-Costraint\"\"\"\n",
    "combined_vecs_lc = [np.append(w2v_vecs_lc[i], pages_tfidf_vecs_lc[i]) for i in range(len(pages_tfidf_vecs_lc))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-SNE\n",
    "Reducing vectors dimensionality to 2 for plot purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, random_state=1)\n",
    "\n",
    "\"\"\"No-Costraint\"\"\"\n",
    "twodim_nc = tsne.fit_transform(combined_vecs_nc)\n",
    "\n",
    "\"\"\"List-Costraint\"\"\"\n",
    "twodim_lc = tsne.fit_transform(combined_vecs_lc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=15)\n",
    "\n",
    "\"\"\"No-Costraint\"\"\"\n",
    "kmeans.fit(tfidf_matrix_nc)\n",
    "kmeans_clusters_nc = kmeans.labels_\n",
    "kmeans_colors_nc = [get_color(i) for i in kmeans_clusters_nc]\n",
    "\n",
    "\"\"\"List-Costraint\"\"\"\n",
    "#kmeans.fit(tfidf_matrix_lc)\n",
    "kmeans.fit(w2v_vecs_lc)\n",
    "kmeans_clusters_lc = kmeans.labels_\n",
    "kmeans_colors_lc = [get_color(i) for i in kmeans_clusters_lc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means No-costraint plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kmeans_data_nc = scatter_plot(twodim_nc, word_labels=longurls_nc, colors=kmeans_colors_nc)\n",
    "py.iplot(kmeans_data_nc, filename=\"K-Means TFIDF-W2V Clustering - No-Costraint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means List-costraint plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kmeans_data_lc = scatter_plot(twodim_lc, word_labels=longurls_lc, colors=kmeans_colors_lc)\n",
    "py.iplot(kmeans_data_lc, filename=\"K-Means TFIDF-W2V Clustering - List-Costraint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDBSCAN Clsustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hdbscan = HDBSCAN(min_cluster_size=7)\n",
    "\n",
    "\"\"\"No-Costraint\"\"\"\n",
    "hdbscan_labels_nc = hdbscan.fit_predict(tfidf_matrix_nc)\n",
    "hdbscan_colors_nc = [get_color(n_clust) for n_clust in hdbscan_labels_nc]\n",
    "\n",
    "print \"Clusters found with HDBSCAN in No-costraint Dataset:\", len(set(hdbscan_labels_nc))\n",
    "print [label for label in set(hdbscan_labels_nc)], \"\\n\"\n",
    "\n",
    "\"\"\"List-Costraint\"\"\"\n",
    "hdbscan_labels_lc = hdbscan.fit_predict(tfidf_matrix_lc)\n",
    "hdbscan_colors_lc = [get_color(n_clust) for n_clust in hdbscan_labels_lc]\n",
    "\n",
    "print \"Clusters found with HDBSCAN in List-costraint Dataset:\", len(set(hdbscan_labels_nc))\n",
    "print [label for label in set(hdbscan_labels_nc)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDBSCAN No-costraint plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hdbscan_data_nc = scatter_plot(twodim_nc, word_labels=longurls_nc, colors=hdbscan_colors_nc)\n",
    "py.iplot(hdbscan_data_nc, filename=\"HDBSCAN TFIDF-W2V Clustering - No-Costraint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDBSCAN List-costraint plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hdbscan_data_lc = scatter_plot(twodim_lc, word_labels=longurls_lc, colors=hdbscan_colors_lc)\n",
    "py.iplot(hdbscan_data_lc, filename=\"HDBSCAN TFIDF-W2V Clustering - List-Costraint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(hdbscan_labels_lc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"real_membership_list = real_membership_lc\n",
    "clusters_found_labels = hdbscan_labels_lc\n",
    "\n",
    "conf_table = np.zeros((len(set(real_membership_list)), len(set(clusters_found_labels))), dtype=\"int8\")\n",
    "real_clusters_set = set(real_membership_list)\n",
    "\n",
    "assert isinstance(real_membership_list[0], int), \"Type is not int\"\n",
    "assert isinstance(clusters_found_labels[0], int), \"Type is not int\"\n",
    "\n",
    "for current_clust in real_clusters_set:\n",
    "        for i in range(len(clusters_found_labels)):\n",
    "            if int(real_membership_list[i]) == int(current_clust):\n",
    "                cluster_found = clusters_found_labels[i]\n",
    "                conf_table[current_clust][cluster_found] = 1 # conf_table[current_clust][cluster_found] + 1\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_confusion_table(real_membership_list, clusters_found_labels):\n",
    "    \n",
    "    assert isinstance(real_membership_list[0], int), \"Type is not int\"\n",
    "    assert isinstance(clusters_found_labels[0], int), \"Type is not int\"\n",
    "    \n",
    "    # matrix(num_of real_clusters x clusters_found)\n",
    "    conf_table = np.zeros((len(set(real_membership_list)), len(set(clusters_found_labels))))\n",
    "    real_clusters_set = set(real_membership_list)\n",
    "    \n",
    "    real_clust_map = {}\n",
    "    index = 0\n",
    "    for c in real_clusters_set:\n",
    "        if not c in real_clust_map:\n",
    "            real_clust_map[c] = index\n",
    "            index += 1\n",
    "    print real_clust_map\n",
    "    \n",
    "    for current_clust in real_clust_map.values():\n",
    "        for i in range(len(clusters_found_labels)):\n",
    "            if real_clust_map[real_membership_list[i]] == current_clust:\n",
    "                cluster_found = clusters_found_labels[i]\n",
    "                conf_table[current_clust, cluster_found] = conf_table[current_clust, cluster_found] + 1\n",
    "    return conf_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "real_membership_lc = [int(m) for m in real_membership_lc]\n",
    "print (set(real_membership_lc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(get_confusion_table(real_membership_lc, [int(i) for i in kmeans_clusters_lc]), index=[0,1,2,3,4,6,8,10,12,13,14,15,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(get_confusion_table(real_membership_lc, [int(i) for i in kmeans_clusters_lc]), index=[0,1,2,3,4,6,8,10,12,13,14,15,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "interest_clusters = [12]\n",
    "for i in range(len(kmeans_clusters_lc)):\n",
    "    value = kmeans_clusters_lc[i]\n",
    "    if int(value) in interest_clusters and real_membership_lc[i]==10:\n",
    "        print(longurls_lc[i], value)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
