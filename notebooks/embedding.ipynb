{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# URL Embedding Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn import metrics\n",
    "from sklearn.manifold import TSNE\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from url2vec.util.plotter import *\n",
    "from url2vec.util.metrics import *\n",
    "from url2vec.util.seqmanager import *\n",
    "\n",
    "import plotly.plotly as py\n",
    "from plotly.graph_objs import *\n",
    "from __future__ import print_function\n",
    "from plotly.tools import FigureFactory as FF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cs.illinois.edu    cs.stanford.edu    eecs.mit.edu    enel.it    cs.princeton.edu    cs.ox.ac.uk\n",
    "site = \"cs.princeton.edu\"\n",
    "words = \"10000\"\n",
    "depth = \"7\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The crawling proccess has been done in two different ways:\n",
    "\n",
    "- **No costraint**: the crawler follows a random outlink from all of the outlinks in a given page\n",
    "- **List costraint**: the crawler follows a random outlink but only from the outlinks in \"lists\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec - no-costraint\n",
    "Word embedding algorithm. Word2vec is a two-layer neural net that processes text. Its input is a text corpus and its output is a set of vectors: feature vectors for words in that corpus. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we're loading all the files that the crawler has generated to train word2vec model.\n",
    "\n",
    "See the [Dataset README](https://github.com/chrisPiemonte/url2vec/tree/master/dataset \"Dataset\") for further information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nocostraint_path = os.getcwd() + \"/../dataset/\" + site + \"/no_constraint/words\" + words + \"_depth\" + depth + \"/\"\n",
    "nocostraint_urlmap_path = nocostraint_path + \"urlsMap.txt\"\n",
    "nocostraint_seq_path = nocostraint_path + \"sequenceIDs.txt\"\n",
    "\n",
    "nocostraint_urlmap = get_urlmap(nocostraint_urlmap_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model. No need to keep everything in RAM so we're passing two generators.\n",
    "\n",
    "**PARAMETERS**:\n",
    "\n",
    "- **min_count**: ignore all words with total frequency lower than this\n",
    "- **window**: is the maximum distance between the current and predicted word within a sentence\n",
    "- **negative**: if > 0, negative sampling will be used, the int for negative specifies how many “noise words” should be drawn (usually between 5-20). Default is 5. If set to 0, no negative samping is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# because of generator\n",
    "vocab_sequences_nc = get_sequences(nocostraint_seq_path)\n",
    "train_sequences_nc = get_sequences(nocostraint_seq_path)\n",
    "\n",
    "%time w2v_model_nc = Word2Vec(min_count=1, window=5, negative=5, sg=1)\n",
    "w2v_model_nc.build_vocab(vocab_sequences_nc)\n",
    "w2v_model_nc.train(train_sequences_nc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url_not_in_sequences = list(set(nocostraint_urlmap) - set(w2v_model_nc.vocab))\n",
    "print(len(url_not_in_sequences))\n",
    "for url in url_not_in_sequences:\n",
    "    del nocostraint_urlmap[url]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE\n",
    "Applying t-SNE for dimensionality reduction. We need two dimensional vectors for visualization purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(nocostraint_urlmap))\n",
    "print(len(w2v_model_nc.vocab))\n",
    "# 100-dim vecs\n",
    "wordvecs_nc = np.array([w2v_model_nc[key] for key in nocostraint_urlmap], dtype=\"float64\")\n",
    "\n",
    "# URL labels\n",
    "urls_nc = [nocostraint_urlmap[key] for key in nocostraint_urlmap]\n",
    "\n",
    "# 2-dim vecs\n",
    "tsne = TSNE(n_components=2)\n",
    "%time twodim_wordvecs_nc = tsne.fit_transform(wordvecs_nc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering - No-Costraint Word2Vec model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN\n",
    "DBSCAN - Density-Based Spatial Clustering of Applications with Noise. Finds core samples of high density and expands clusters from them. Good for data which contains clusters of similar density.\n",
    "\n",
    "**PARAMETERS**:\n",
    "\n",
    "- **eps** : The maximum distance between two samples for them to be considered as in the same neighborhood.\n",
    "- **min_samples** : The number of samples (or total weight) in a neighborhood for a point to be considered as a core point. This includes the point itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps=1.0, min_samples=10)\n",
    "%time dbscan.fit(wordvecs_nc)\n",
    "dbscan_labels_nc = dbscan.labels_\n",
    "\n",
    "dbscan_colors_nc = [get_color(clust) for clust in dbscan_labels_nc]\n",
    "\n",
    "print(\"Clusters found with DBSCAN:\", len(set(dbscan_labels_nc)))\n",
    "print ([label for label in set(dbscan_labels_nc)])\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dbscan_data_nc = scatter_plot(twodim_wordvecs_nc, urls_nc, dbscan_colors_nc)\n",
    "py.iplot(dbscan_data_nc, filename='Word Vectors Nocostraint - Scatter plot DBSCAN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <a href=\"https://plot.ly/~chrispolo/0\" \n",
    "        target=\"_blank\" title=\"y\" \n",
    "        style=\"display: none; text-align: center;\">\n",
    "            <img src=\"../dataset/img/nc_wordvectors_scatter_plot_DBSCAN.png\" \n",
    "                alt=\"y\" style=\"max-width: 100%;width: 1121px;\"  \n",
    "                width=\"100%\" onerror=\"this.onerror=null;this.src='https://plot.ly/404';\" />\n",
    "    </a>\n",
    "    <script data-plotly=\"chrispolo:0\"  src=\"https://plot.ly/embed.js\" async></script>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDBSCAN\n",
    "HDBSCAN - Hierarchical Density-Based Spatial Clustering of Applications with Noise. Performs DBSCAN over varying epsilon values and integrates the result to find a clustering that gives the best stability over epsilon. This allows HDBSCAN to find clusters of varying densities (unlike DBSCAN), and be more robust to parameter selection.\n",
    "\n",
    "**PARAMETERS**:\n",
    "- **min_cluster_size** : minimum nodes to form a cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hdbscan = HDBSCAN(min_cluster_size=15)\n",
    "%time hdbscan_labels_nc = hdbscan.fit_predict(wordvecs_nc)\n",
    "\n",
    "hdbscan_colors_nc = [get_color(clust) for clust in hdbscan_labels_nc]\n",
    "\n",
    "print(\"Clusters found with HDBSCAN:\", len(set(hdbscan_labels_nc)))\n",
    "print([label for label in set(hdbscan_labels_nc)])\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDBSCAN Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hdbscan_data_nc = scatter_plot(twodim_wordvecs_nc, urls_nc, hdbscan_colors_nc)\n",
    "py.iplot(hdbscan_data_nc, filename='Word Vectors Nocostraint - Scatter plot HDBSCAN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <a href=\"https://plot.ly/~chrispolo/2\" \n",
    "        target=\"_blank\" title=\"y\" \n",
    "        style=\"display: none; text-align: center;\">\n",
    "            <img src=\"../dataset/img/nc_wordvectors_scatter_plot_HDBSCAN.png\" \n",
    "                alt=\"y\" style=\"max-width: 100%;width: 1121px;\"  \n",
    "                width=\"100%\" onerror=\"this.onerror=null;this.src='https://plot.ly/404';\" />\n",
    "    </a>\n",
    "    <script data-plotly=\"chrispolo:2\"  src=\"https://plot.ly/embed.js\" async></script>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-MEANS\n",
    "The first step is to initialize the algorithm by choosing K initial cluster centroid locations. This is typically done by randomly choosing K points from the input set. With these initial K centroids, the algorithm proceeds by repeating the following two main steps:\n",
    "\n",
    "- Cluster Assignment - Here, each observation (e.g., each point in the data set) is assigned to a cluster centroid such that the WCSS objective function is minimized. This can often be translated to assigning each observation to the closest cluster centroid (which coincidentally minimizes WCSS for many distance metrics), though for some distance metrics and spaces this need not be the case.\n",
    "- Update Centroids - After all of the input observations have been assigned to a cluster centroid, each centroid is re-computed. For each cluster, the new centroid is computed by averaging the observations that were assigned to it (e.g., computing the 'mean' of the observations).\n",
    "\n",
    "These steps are repeated until the algorithm \"converges\".\n",
    "\n",
    "**PARAMETERS**:\n",
    "\n",
    "- **n_clusters**: number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=10)\n",
    "%time kmeans.fit(wordvecs_nc)\n",
    "\n",
    "kmeans_labels_nc = kmeans.labels_\n",
    "\n",
    "kmeans_colors_nc = [get_color(clust) for clust in kmeans_labels_nc]\n",
    "\n",
    "print(\"Clusters found with K-MEANS:\", len(set(kmeans_labels_nc)))\n",
    "print([label for label in set(kmeans_labels_nc)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-MEANS Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "kmeans_data_nc = scatter_plot(twodim_wordvecs_nc, urls_nc, kmeans_colors_nc)\n",
    "py.iplot(kmeans_data_nc, filename='Word Vectors Nocostraint - Scatter plot K-MEANS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <a href=\"https://plot.ly/~chrispolo/4\" \n",
    "        target=\"_blank\" title=\"y\" \n",
    "        style=\"display: none; text-align: center;\">\n",
    "            <img src=\"../dataset/img/nc_wordvectors_scatter_plot_KMEANS.png\" \n",
    "                alt=\"y\" style=\"max-width: 100%;width: 1121px;\"  \n",
    "                width=\"100%\" onerror=\"this.onerror=null;this.src='https://plot.ly/404';\" />\n",
    "    </a>\n",
    "    <script data-plotly=\"chrispolo:4\"  src=\"https://plot.ly/embed.js\" async></script>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GROUND TRUTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gt = GroundTruth(os.getcwd() + \"/../dataset/\" + site + \"/ground_truth/urlToMembership.txt\")\n",
    "ground_truth_nc = [int(gt.get_groundtruth(nocostraint_urlmap[key])) for key in nocostraint_urlmap]\n",
    "\n",
    "real_colors_nc = [get_color(n) for n in ground_truth_nc]\n",
    "\n",
    "print(\"Clusters found manually:\", len(set(ground_truth_nc)))\n",
    "print([label for label in set(ground_truth_nc)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GROUND TRUTH Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "groundtruth_data_nc = scatter_plot(twodim_wordvecs_nc, urls_nc, real_colors_nc)\n",
    "py.iplot(groundtruth_data_nc, filename='Word Vectors Nocostraint - Scatter plot Ground Truth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <a href=\"https://plot.ly/~chrispolo/56\" \n",
    "        target=\"_blank\" title=\"y\" \n",
    "        style=\"display: none; text-align: center;\">\n",
    "            <img src=\"../dataset/img/nc_wordvectors_scatter_plot_GROUNDTRUTH.png\" \n",
    "                alt=\"y\" style=\"max-width: 100%;width: 1121px;\"  \n",
    "                width=\"100%\" onerror=\"this.onerror=null;this.src='https://plot.ly/404';\" />\n",
    "    </a>\n",
    "    <script data-plotly=\"chrispolo:56\"  src=\"https://plot.ly/embed.js\" async></script>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec - list-costraint\n",
    "Word embedding alorithm. Word2vec is a two-layer neural net that processes text. Its input is a text corpus and its output is a set of vectors: feature vectors for words in that corpus. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we're loading all the files that the crawler has generated to train word2vec model.\n",
    "\n",
    "See the [Dataset README](https://github.com/chrisPiemonte/url2vec/tree/master/dataset \"Dataset\") for further information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "listcostraint_path        = os.getcwd() + \"/../dataset/\" + site + \"/list_constraint/words\" + words + \"_depth\" + depth + \"/\"\n",
    "listcostraint_urlmap_path = listcostraint_path + \"urlsMap.txt\"\n",
    "listcostraint_seq_path    = listcostraint_path + \"sequenceIDs.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model. No need to keep everything in RAM so we're passing two generators.\n",
    "\n",
    "**PARAMETERS**:\n",
    "\n",
    "- **min_count**: ignore all words with total frequency lower than this\n",
    "- **window**: is the maximum distance between the current and predicted word within a sentence\n",
    "- **negative**: if > 0, negative sampling will be used, the int for negative specifies how many “noise words” should be drawn (usually between 5-20). Default is 5. If set to 0, no negative samping is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "913591"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# because of generator\n",
    "vocab_seq_lc = get_sequences(listcostraint_seq_path)\n",
    "train_seq_lc = get_sequences(listcostraint_seq_path)\n",
    "\n",
    "%time word2vec_lc = Word2Vec(min_count=1, window=5, negative=5, sg=1)\n",
    "word2vec_lc.build_vocab(vocab_seq_lc)\n",
    "word2vec_lc.train(train_seq_lc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE\n",
    "Applying t-SNE for dimensionality reduction. We need two dimensional vectors for visualization purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "listcostraint_urlmap = get_urlmap(listcostraint_urlmap_path, sep=\" , \")\n",
    "\n",
    "url_not_in_sequences = list(set(listcostraint_urlmap) - set(word2vec_lc.vocab))\n",
    "print(len(url_not_in_sequences))\n",
    "for url in url_not_in_sequences:\n",
    "    del listcostraint_urlmap[url]\n",
    "\n",
    "# 100-dim vecs\n",
    "wordvecs_lc = np.array([word2vec_lc[key] for key in listcostraint_urlmap], dtype=\"float64\")\n",
    "\n",
    "# URL labels\n",
    "urls_lc = [listcostraint_urlmap[key] for key in listcostraint_urlmap]\n",
    "\n",
    "# 2-dim vecs\n",
    "tsne = TSNE(n_components=2)\n",
    "%time twodim_wordvecs_lc = tsne.fit_transform(wordvecs_lc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN\n",
    "DBSCAN - Density-Based Spatial Clustering of Applications with Noise. Finds core samples of high density and expands clusters from them. Good for data which contains clusters of similar density.\n",
    "\n",
    "**PARAMETERS**:\n",
    "\n",
    "- **eps** : The maximum distance between two samples for them to be considered as in the same neighborhood.\n",
    "- **min_samples** : The number of samples (or total weight) in a neighborhood for a point to be considered as a core point. This includes the point itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clusters found with DBSCAN: 15\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, -1]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "%time dbscan.fit(wordvecs_lc)\n",
    "dbscan_labels_lc = dbscan.labels_\n",
    "\n",
    "dbscan_colors_lc = [get_color(clust) for clust in dbscan_labels_lc]\n",
    "\n",
    "print(\"Clusters found with DBSCAN:\", len(set(dbscan_labels_lc)))\n",
    "print ([label for label in set(dbscan_labels_lc)])\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\"seamless=\"seamless\" src=\"https://plot.ly/~chrispolo/60.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbscan_data_lc = scatter_plot(twodim_wordvecs_lc, urls_lc, dbscan_colors_lc)\n",
    "py.iplot(dbscan_data_lc, filename='Word Vectors Lists - DBSCAN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <a href=\"https://plot.ly/~chrispolo/60\" \n",
    "        target=\"_blank\" title=\"y\" \n",
    "        style=\"display: none; text-align: center;\">\n",
    "            <img src=\"../dataset/img/lc_wordvectors_scatter_plot_DBSCAN.png\" \n",
    "                alt=\"y\" style=\"max-width: 100%;width: 1121px;\"  \n",
    "                width=\"100%\" onerror=\"this.onerror=null;this.src='https://plot.ly/404';\" />\n",
    "    </a>\n",
    "    <script data-plotly=\"chrispolo:60\"  src=\"https://plot.ly/embed.js\" async></script>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDBSCAN\n",
    "HDBSCAN - Hierarchical Density-Based Spatial Clustering of Applications with Noise. Performs DBSCAN over varying epsilon values and integrates the result to find a clustering that gives the best stability over epsilon. This allows HDBSCAN to find clusters of varying densities (unlike DBSCAN), and be more robust to parameter selection.\n",
    "\n",
    "**PARAMETERS**:\n",
    "- **min_cluster_size** : minimum nodes to form a cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clusters found with HDBSCAN: 45\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, -1]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hdbscan = HDBSCAN(min_cluster_size=5)\n",
    "%time hdbscan_labels_lc = hdbscan.fit_predict(wordvecs_lc)\n",
    "\n",
    "hdbscan_colors_lc = [get_color(clust) for clust in hdbscan_labels_lc]\n",
    "\n",
    "print(\"Clusters found with HDBSCAN:\", len(set(hdbscan_labels_lc)))\n",
    "print([label for label in set(hdbscan_labels_lc)])\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDBSCAN Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\"seamless=\"seamless\" src=\"https://plot.ly/~chrispolo/62.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdbscan_data_lc = scatter_plot(twodim_wordvecs_lc, urls_lc, hdbscan_colors_lc)\n",
    "py.iplot(hdbscan_data_lc, filename='Word Vectors Lists - Scatter plot HDBSCAN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <a href=\"https://plot.ly/~chrispolo/62\" \n",
    "        target=\"_blank\" title=\"y\" \n",
    "        style=\"display: none; text-align: center;\">\n",
    "            <img src=\"../dataset/img/lc_wordvectors_scatter_plot_HDBSCAN.png\" \n",
    "                alt=\"y\" style=\"max-width: 100%;width: 1121px;\"  \n",
    "                width=\"100%\" onerror=\"this.onerror=null;this.src='https://plot.ly/404';\" />\n",
    "    </a>\n",
    "    <script data-plotly=\"chrispolo:62\"  src=\"https://plot.ly/embed.js\" async></script>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-MEANS\n",
    "The first step is to initialize the algorithm by choosing K initial cluster centroid locations. This is typically done by randomly choosing K points from the input set. With these initial K centroids, the algorithm proceeds by repeating the following two main steps:\n",
    "\n",
    "- Cluster Assignment - Here, each observation (e.g., each point in the data set) is assigned to a cluster centroid such that the WCSS objective function is minimized. This can often be translated to assigning each observation to the closest cluster centroid (which coincidentally minimizes WCSS for many distance metrics), though for some distance metrics and spaces this need not be the case.\n",
    "- Update Centroids - After all of the input observations have been assigned to a cluster centroid, each centroid is re-computed. For each cluster, the new centroid is computed by averaging the observations that were assigned to it (e.g., computing the 'mean' of the observations).\n",
    "\n",
    "These steps are repeated until the algorithm \"converges\".\n",
    "\n",
    "**PARAMETERS**:\n",
    "\n",
    "- **n_clusters**: number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clusters found with K-MEANS: 15\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=15)\n",
    "%time kmeans.fit(wordvecs_lc)\n",
    "\n",
    "kmeans_labels_lc = kmeans.labels_\n",
    "\n",
    "kmeans_colors_lc = [get_color(clust) for clust in kmeans_labels_lc]\n",
    "\n",
    "print(\"Clusters found with K-MEANS:\", len(set(kmeans_labels_lc)))\n",
    "print([label for label in set(kmeans_labels_lc)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-MEANS Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\"seamless=\"seamless\" src=\"https://plot.ly/~chrispolo/64.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans_data_lc = scatter_plot(twodim_wordvecs_lc, urls_lc, kmeans_colors_lc)\n",
    "py.iplot(kmeans_data_lc, filename='Word Vectors Lists - Scatter plot K-MEANS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <a href=\"https://plot.ly/~chrispolo/64\" \n",
    "        target=\"_blank\" title=\"y\" \n",
    "        style=\"display: none; text-align: center;\">\n",
    "            <img src=\"../dataset/img/lc_wordvectors_scatter_plot_KMEANS.png\" \n",
    "                alt=\"y\" style=\"max-width: 100%;width: 1121px;\"  \n",
    "                width=\"100%\" onerror=\"this.onerror=null;this.src='https://plot.ly/404';\" />\n",
    "    </a>\n",
    "    <script data-plotly=\"chrispolo:64\"  src=\"https://plot.ly/embed.js\" async></script>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GROUND TRUTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: '/home/chris/workspace/jupyter-notebook/url2vec/notebooks/../dataset/cs.princeton.edu/ground_truth/urlToMembership.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-ece4374e63ea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGroundTruth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/../dataset/\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msite\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/ground_truth/urlToMembership.txt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mground_truth_lc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_groundtruth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlistcostraint_urlmap\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlistcostraint_urlmap\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mreal_colors_lc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mget_color\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mground_truth_lc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/chris/workspace/jupyter-notebook/url2vec/url2vec/util/seqmanager.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, fpath, sep)\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[1;31m# constructor, needs the file path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mground_truth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfpath\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[1;31m# returns the real cluster membership of a URL\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: [Errno 2] No such file or directory: '/home/chris/workspace/jupyter-notebook/url2vec/notebooks/../dataset/cs.princeton.edu/ground_truth/urlToMembership.txt'"
     ]
    }
   ],
   "source": [
    "gt = GroundTruth(os.getcwd() + \"/../dataset/\" + site + \"/ground_truth/urlToMembership.txt\")\n",
    "ground_truth_lc = [int(gt.get_groundtruth(listcostraint_urlmap[key])) for key in listcostraint_urlmap]\n",
    "\n",
    "real_colors_lc = [get_color(n) for n in ground_truth_lc]\n",
    "\n",
    "print(\"Clusters found manually:\", len(set(ground_truth_lc)))\n",
    "print([label for label in set(ground_truth_lc)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GROUND TRUTH Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "groundtruth_data_lc = scatter_plot(twodim_wordvecs_lc, urls_lc, real_colors_lc)\n",
    "py.iplot(groundtruth_data_lc, filename='Word Vectors Lists - Scatter plot Ground Truth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <a href=\"https://plot.ly/~chrispolo/66\" \n",
    "        target=\"_blank\" title=\"y\" \n",
    "        style=\"display: none; text-align: center;\">\n",
    "            <img src=\"../dataset/img/lc_wordvectors_scatter_plot_GROUNDTRUTH.png\" \n",
    "                alt=\"y\" style=\"max-width: 100%;width: 1121px;\"  \n",
    "                width=\"100%\" onerror=\"this.onerror=null;this.src='https://plot.ly/404';\" />\n",
    "    </a>\n",
    "    <script data-plotly=\"chrispolo:66\"  src=\"https://plot.ly/embed.js\" async></script>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analogies\n",
    "Training the model allows some operations like the famous \"king – man + woman = queen“:\n",
    "\n",
    "Here we are providing the System Programming course **(33)** and its instructor **(253)**, and subtracting another course, Computer Architecture **(32)**\n",
    "\n",
    "33:  http://cs.illinois.edu/courses/profile/CS241-120158\n",
    "\n",
    "253: http://cs.illinois.edu/directory/profile/angrave\n",
    "\n",
    "32:  http://cs.illinois.edu/courses/profile/CS233-120158\n",
    "\n",
    "and expecting its instructor: http://cs.illinois.edu/directory/profile/zilles **(251)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# hand is to palm as foot ____ (it's sole if you're wondering)\n",
    "# HAND : PALM : : FOOT : ____   \n",
    "hand = \"33\"\n",
    "palm = \"253\"\n",
    "foot = \"32\"\n",
    "\n",
    "most_similar_list = word2vec_lc.most_similar(positive=[hand, palm], negative=[foot], topn=1)\n",
    "sole = most_similar_list[0][0]\n",
    "\n",
    "print(listcostraint_urlmap[hand].replace(\"http://cs.illinois.edu/courses/profile/\", \"\"), \"is to\", \n",
    "      listcostraint_urlmap[palm].replace(\"http://cs.illinois.edu/directory/profile/\", \"\"), \"as\",\n",
    "      listcostraint_urlmap[foot].replace(\"http://cs.illinois.edu/courses/profile/\", \"\"), \"is to\",\n",
    "      listcostraint_urlmap[sole].replace(\"http://cs.illinois.edu/directory/profile/\", \"\")\n",
    ")\n",
    "\n",
    "print(\"\")\n",
    "print(hand, listcostraint_urlmap[hand])\n",
    "print(palm, listcostraint_urlmap[palm])\n",
    "print(foot, listcostraint_urlmap[foot])\n",
    "print(sole, listcostraint_urlmap[sole])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doesn't match\n",
    "Word that doesn't go with the others:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Artificial Intelligence research lab:\n",
    "- 128 http://cs.illinois.edu/research/artificial-intelligence\n",
    "\n",
    "Its researchers:\n",
    "- 304 http://cs.illinois.edu/directory/profile/mrebl\n",
    "- 271 http://cs.illinois.edu/directory/profile/daf\n",
    "- 305 http://cs.illinois.edu/directory/profile/juliahmr\n",
    "- 363 http://cs.illinois.edu/directory/profile/dhoiem\n",
    "\n",
    "Another guy:\n",
    "- 361 http://cs.illinois.edu/directory/profile/wgropp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word2vec_lc.doesnt_match(\"304 271 305 361 363\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Evaluating the performance of a clustering algorithm is not as trivial as counting the number of errors or the precision and recall of a supervised classification algorithm. In particular any evaluation metric should not take the absolute values of the cluster labels into account but rather if this clustering define separations of the data similar to some ground truth set of classes or satisfying some assumption such that members belong to the same class are more similar that members of different classes according to some similarity metric.\n",
    "\n",
    "See the [scikit-learn documentaion](http://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation \"ti\") for futher information\n",
    "\n",
    "### Metrics:\n",
    "\n",
    "- **Homogeneity**: each cluster contains only members of a single class\n",
    "\n",
    "\n",
    "- **Completeness**: all members of a given class are assigned to the same cluster\n",
    "\n",
    "\n",
    "- **Adjusted Rand index**: Given the knowledge of the *ground truth* class assignments and our clustering algorithm assignments of the same samples, the adjusted Rand index is a function that measures the similarity of the two assignments, ignoring permutations and with chance normalization\n",
    "\n",
    "\n",
    "- **V-measure**: The V-measure is actually equivalent to the mutual information (NMI) discussed above normalized by the sum of the label entropies\n",
    "\n",
    "\n",
    "- **Mutual Information based scores**: Given the knowledge of the ground truth class assignments and our clustering algorithm assignments of the same samples, the Mutual Information is a function that measures the agreement of the two assignments, ignoring permutations. Two different normalized versions of this measure are available, Normalized Mutual Information(NMI) and Adjusted Mutual Information(AMI). NMI is often used in the literature while AMI was proposed more recently and is normalized against chance\n",
    "\n",
    "\n",
    "- **Silhouette**: If the ground truth labels are not known, evaluation must be performed using the model itself. The Silhouette Coefficient is an example of such an evaluation, where a higher Silhouette Coefficient score relates to a model with better defined clusters. The score is bounded between -1 for incorrect clustering and +1 for highly dense clustering. Scores around zero indicate overlapping clusters. The score is higher when clusters are dense and well separated, which relates to a standard concept of a cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metrics_df = pd.DataFrame([\n",
    "        [\n",
    "            # dbscan nocostraint\n",
    "            metrics.homogeneity_score(ground_truth_nc, dbscan_labels_nc),\n",
    "            metrics.completeness_score(ground_truth_nc, dbscan_labels_nc),\n",
    "            metrics.v_measure_score(ground_truth_nc, dbscan_labels_nc),\n",
    "            metrics.adjusted_rand_score(ground_truth_nc, dbscan_labels_nc),\n",
    "            metrics.adjusted_mutual_info_score(ground_truth_nc, dbscan_labels_nc),\n",
    "            metrics.silhouette_score(wordvecs_nc, dbscan_labels_nc, metric='euclidean')\n",
    "        ],\n",
    "        [\n",
    "            # hdbscan nocostraint\n",
    "            metrics.homogeneity_score(ground_truth_nc, hdbscan_labels_nc),\n",
    "            metrics.completeness_score(ground_truth_nc, hdbscan_labels_nc),\n",
    "            metrics.v_measure_score(ground_truth_nc, hdbscan_labels_nc),\n",
    "            metrics.adjusted_rand_score(ground_truth_nc, hdbscan_labels_nc),\n",
    "            metrics.adjusted_mutual_info_score(ground_truth_nc, hdbscan_labels_nc),\n",
    "            metrics.silhouette_score(wordvecs_nc, hdbscan_labels_nc, metric='euclidean')\n",
    "        ],\n",
    "        [\n",
    "            # kmeans nocostraint\n",
    "            metrics.homogeneity_score(ground_truth_nc, kmeans_labels_nc),\n",
    "            metrics.completeness_score(ground_truth_nc, kmeans_labels_nc),\n",
    "            metrics.v_measure_score(ground_truth_nc, kmeans_labels_nc),\n",
    "            metrics.adjusted_rand_score(ground_truth_nc, kmeans_labels_nc),\n",
    "            metrics.adjusted_mutual_info_score(ground_truth_nc, kmeans_labels_nc),\n",
    "            metrics.silhouette_score(wordvecs_nc, kmeans_labels_nc, metric='euclidean')\n",
    "        ],\n",
    "        [\n",
    "            # dbscan listcostraint\n",
    "            metrics.homogeneity_score(ground_truth_lc, dbscan_labels_lc),\n",
    "            metrics.completeness_score(ground_truth_lc, dbscan_labels_lc),\n",
    "            metrics.v_measure_score(ground_truth_lc, dbscan_labels_lc),\n",
    "            metrics.adjusted_rand_score(ground_truth_lc, dbscan_labels_lc),\n",
    "            metrics.adjusted_mutual_info_score(ground_truth_lc, dbscan_labels_lc),\n",
    "            metrics.silhouette_score(wordvecs_lc, dbscan_labels_lc, metric='euclidean')\n",
    "        ],\n",
    "        [\n",
    "            # hdbscan listcostraint\n",
    "            metrics.homogeneity_score(ground_truth_lc, hdbscan_labels_lc),\n",
    "            metrics.completeness_score(ground_truth_lc, hdbscan_labels_lc),\n",
    "            metrics.v_measure_score(ground_truth_lc, hdbscan_labels_lc),\n",
    "            metrics.adjusted_rand_score(ground_truth_lc, hdbscan_labels_lc),\n",
    "            metrics.adjusted_mutual_info_score(ground_truth_lc, hdbscan_labels_lc),\n",
    "            metrics.silhouette_score(wordvecs_lc, hdbscan_labels_lc, metric='euclidean')\n",
    "        ],\n",
    "        [\n",
    "            # kmeans listcostraint\n",
    "            metrics.homogeneity_score(ground_truth_lc, kmeans_labels_lc),\n",
    "            metrics.completeness_score(ground_truth_lc, kmeans_labels_lc),\n",
    "            metrics.v_measure_score(ground_truth_lc, kmeans_labels_lc),\n",
    "            metrics.adjusted_rand_score(ground_truth_lc, kmeans_labels_lc),\n",
    "            metrics.adjusted_mutual_info_score(ground_truth_lc, kmeans_labels_lc),\n",
    "            metrics.silhouette_score(wordvecs_lc, kmeans_labels_lc, metric='euclidean')\n",
    "        ]],\n",
    "        index=[\n",
    "            \"NoCostraint - DBSCAN\", \n",
    "            \"NoCostraint - HDBSCAN\", \n",
    "            \"NoCostraint - K-MEANS\", \n",
    "            \"ListCostraint - DBSCAN\", \n",
    "            \"ListCostraint - HDBSCAN\", \n",
    "            \"ListCostraint - K-MEANS\"\n",
    "        ],\n",
    "        columns=[\n",
    "            \"Homogeneity\", \n",
    "            \"Completeness\", \n",
    "            \"V-Measure core\", \n",
    "            \"Adjusted Rand index\", \n",
    "            \"Mutual Information\",\n",
    "            \"Silhouette\"\n",
    "        ])\n",
    "\n",
    "metrics_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
