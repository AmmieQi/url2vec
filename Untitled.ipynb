{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Path of real membership file: ', '/home/chris/workspace/jupyter-notebook/url2vec/../dataset/manual-membership/urlToMembership.txt')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from url_sequences.sequence_manager import *\n",
    "from url_sequences.sequence_plotter import *\n",
    "from url_sequences.sequence_handler import *\n",
    "from url_sequences.clustering_metrics import *\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Url2Vec:\n",
    "    # Real Membership default file path\n",
    "    \n",
    "    # doesn't work on ipython because of __file__\n",
    "    #_rmfile = os.path.abspath(os.path.dirname(__file__)) + \"/dataset/manual-membership/urlToMembership.txt\"\n",
    "    _rmfile = \"/home/chris/workspace/jupyter-notebook/url2vec/dataset/manual-membership/urlToMembership.txt\"\n",
    "    \n",
    "    def __init__ (self, map_file, vertex_file, realmembership_file=_rmfile):\n",
    "        self.codecontent_map = get_content_map(vertex_file)\n",
    "        self.codeurl_map = get_urlmap(map_file)\n",
    "        rm = RealMembership(fpath=realmembership_file)\n",
    "        self.real_membership_list = [int(rm.get_membership(self.codeurl_map[code])) for code in self.codecontent_map]\n",
    "    \n",
    "    \n",
    "    def word_embedding(self, sequences_file, vecs_length=48):\n",
    "        vocab_sequences = get_seq(sequences_file, 1)\n",
    "        train_sequences = get_seq(sequences_file, 1)\n",
    "        \n",
    "        w2v_model = Word2Vec(min_count=1, negative=5, size=vecs_length)\n",
    "        w2v_model.build_vocab(vocab_sequences)\n",
    "        w2v_model.train(train_sequences)\n",
    "        return np.array([w2v_model[code] for code in self.codecontent_map])\n",
    "        \n",
    "        \n",
    "    def tfidf(self, vecs_length=50, tfidf=True):\n",
    "        self.pages_content = [self.codecontent_map[code] for code in self.codecontent_map]\n",
    "        self.codes = [code for code in self.codecontent_map]\n",
    "        self.longurls = [self.codeurl_map[code] for code in self.codecontent_map]\n",
    "        \n",
    "        tfidf_vectorizer = TfidfVectorizer(\n",
    "            max_df = 0.8,\n",
    "            max_features = 200000,\n",
    "            min_df = 0.1,\n",
    "            stop_words = 'english',\n",
    "            use_idf = tfidf,\n",
    "            tokenizer = tokenize_and_stem,\n",
    "            ngram_range = (1,3)\n",
    "        )\n",
    "        tfidf_matrix = tfidf_vectorizer.fit_transform(self.pages_content)\n",
    "        svd = TruncatedSVD(n_components=vecs_length, algorithm=\"arpack\", random_state=1)\n",
    "        return svd.fit_transform(tfidf_matrix)\n",
    "    \n",
    "    \n",
    "    def train(self, algorithm=\"kmeans\", use_w2v=True, use_tfidf=True, w2v_size=48, tfidf_size=50, sequences_file=\"\"):\n",
    "        empty_array = np.array([ [] for i in range(len(self.codecontent_map)) ])\n",
    "        w2v_vecs = word_embedding(sequences_file, vecs_length=w2v_size) if use_w2v else empty_array\n",
    "        tfidf_vecs = tfidf_vecs = tfidf(vecs_length=tfidf_size) if use_tfidf else empty_array\n",
    "        \n",
    "        return [ np.append(w2v_vecs[i], tfidf_vecs[i]) for i in range(len(self.codecontent_map)) ]\n",
    "            \n",
    "        \n",
    "    def test(self):\n",
    "        te = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nocostraint_path   = os.getcwd() + \"/dataset/new/cs.illinois.eduNoConstraint.words1000.depth.10/\"\n",
    "vertex_nc_path     = nocostraint_path + \"vertex.txt\"\n",
    "map_nc_path        = nocostraint_path + \"urlsMap.txt\"\n",
    "sequences_nc       = nocostraint_path + \"sequenceIDs.txt\"\n",
    "u2v = Url2Vec(map_nc_path, vertex_nc_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "728"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = u2v.tfidf()\n",
    "vec2 = u2v.word_embedding(sequences_nc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full = np.array([[5,6], [7,8]])\n",
    "\n",
    "empty = np.array([ [] for i in range(len(full)) ])\n",
    "len(np.append(full[0], empty[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
